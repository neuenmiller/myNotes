\chapter{Orthogonality}

Two vectors are orthogonal (translated as right-angled from Greek) when their dot product is zero: \(v \cdot w = v^{T}w = 0\). The vectors in the two subspaces, the vectors in a basis, the column vectors in Q. Orthogonal vectors also have a curious behavior where it is also like pythagoras's theoreom. 
\[
    v^{T}w = 0 \text{ and } \left\lVert v \right\rVert^2 + \left\lVert w \right\rVert^2 = \left\lVert v + w \right\lVert^2
\] 

Important part, \textbf{the fundamental subspaces are orthogonal}
\begin{enumerate}
    \item N(A) contains all vectors orthogonal to the row space \(C(A^T)\).
    \item \(N(A^T)\)  contains all vectors orthogonal to the column space \(C(A)\). 
\end{enumerate} 
\(Ax = 0\) makes \(x\) orthogonal to each row, \(A^{T}y = 0\) make \(y\) orthogonal to each column. 

A key idea in this chapter is \textbf{projection}: If \(b\) is outside the column space of \(A\), then we must find the closest point \(p\) that is still inside. The line from \(b\) to \(p\) shows the error \(e\), and that line is perpendicular to the column space.    THe \textbf{least squares equation} \(A^{T}Ax = A^{T}b\) produces the closest \(p = Ax\) and smallest possible \(e\), it also gives the best \(e\) when \(Ax = b\) is unsolvable. The best \(x\) makes \(\left\lVert Ax - b \right\lVert\) as small as possible, the least squares. \(A^{T}Ax = A^{T}b\) is easy when \(A^{T}A = I\). Then \(A\) has orthonormal columns perpendicular unit vector. Remember that \(Q\) has \(Q^{T}Q = I\) and \(QR = A\), where the R is upper triangle. Orthogonal matrices are perfect for computations, \(A = QR\) is even better than \(A+LU\)               

\section{Orthogonality of Vectors and Subspaces}

\begin{enumerate}
    \item Orthogonal vectors have \(v^{T}w = 0\), then \(\left\lVert v \right\lVert^2\) + \(\left\lVert w \right\lVert\) = \(\left\lVert v + w \right\lVert^2\) as in \(a^2 + b^2 = c^2\)
    \item Subspace \(V\) and \(W\) are orthogonal when \(v^{T}w = 0\) for every \(v\) in \(V\) and every \(w\) in \(W\). 
    \item Row space of \(A\) is orthogonal to nullspace, column space of \(A\) is orthogonal to left nullspace.    
    \item The dimensions add to \(r + (n - r) = n\) and \(r + (m - r) = m\): orthogonal complements. 
    \item If \(n\) vectors in \(R^n\) are independent, they span \(R^n\). If \(n\) vectors span \(R^n\), they are independent.                 
\end{enumerate}

How can we proof that the entirety of the nullspace of \(A\) is orthogonal to the row space of \(A\)? Look at \(Ax = 0\). 
\[
    Ax = 
    \begin{bmatrix}
         \text{row 1 of \(A\) } \\
         \vdots \\
         \text{row $m$ of $A$} \\
    \end{bmatrix}
    \begin{bmatrix}
         x \\
    \end{bmatrix}
    =
    \begin{bmatrix}
         0 \\
         \vdots \\
         0 \\
    \end{bmatrix}
\]  
Notice how they are all zero? That means all of the row space is orthogonal to the nullspace. 

But given that we like to work with columns, another way you can proof it is: \( x^{T}(A^{T}y) = (Ax)^{T}y = 0^{T}y = 0 \)

\textbf{Importantly}, column space \(C(A)\) and left nullspace \(N(A^T)\) is also a perpendicular pair. The proof is more or less the same except we use \(A^T\) instead of \(A\). 

\textbf{Example 1:} The two rows of \(A\) are perpendicular to \(x\) in the nullspace of \(A\): 
\[
    Ax = 
    \begin{bmatrix}
        1 & -2 & 1  \\
        1 & 0 & -1  \\
    \end{bmatrix}
    \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         0 \\
         0 \\
    \end{bmatrix}
\]
or
\[
    A^{T}y = 
    \begin{bmatrix}
        1 & 1  \\
        -2 & 0 \\
        1 & -1  \\
    \end{bmatrix}
    \begin{bmatrix}
         0 \\
         0 \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         0 \\
         0 \\
         0 \\
    \end{bmatrix}
\]   

This is a fairly extreme case since \(C(A)\) is all of  \(R^2\) and the nullspace of \(A^T\) is a zero vector. The total dimension is \(2 + 1 = 3\). The subspaces accounted for all vectors in \(R^3 = R^n\). The column space and left nullspace have a dimension of \(2 + 0 = 2\), accounting for all vectors in \(R^{2} = R^m\). 

Note that \textbf{if V and W are orthogonal subspaces in \(R^{n} \) then \(\dim V + \dim W \leq n\)    }
Of course, imagine our house. We got a wall and a floor, they can't be orthogonal subspaces since they are both \(R^2\) and our world is \(R^3\), \(2 + 2 \geq 3\). So this is wrong. Some vector will lie in both the wall and the floor, which is the line where the wall meets the floor in both subspaces. 

Two orthogonal subspaces that account for the whole space have a special name, \textbf{orthogonal complements}. Orthogonal complement of \(V^{\perp}\) of \(V\) contains all vectors orthogonal to \(V\).    
So the two pairs of subspace in linear algebra are actually orthogonal complements. 
\begin{enumerate}
    \item Row space and null space, \(r + ( n - r) = n\)
    \item Column space and left nullspace \(r + (m - r)= m\)  
\end{enumerate}

Any vector \(x\) in \(R^n\) is the sum \(x = x_{\text{row}} + x_{\text{null}}\) of its row space and component and its null space component. Same goes for \(y\) in \(R^m\) is the sum \(y = y_{\text{col}} + y_{\text{null}}\), between its column space component and its component in \(N(A^T)\).

\textbf{Fundamental theoreom of Linear Algebra, Part 2:
\begin{enumerate}
    \item \(N(A)\) is the orthogonal complement of the row space \(C(A^T)\)  in \(R^n\)
    \item \(N(A^T)\) is the orthogonal complement of the column space \(C(A)\) in \(R^m\)    
\end{enumerate}} 


Check the figure on page 146 of Strang's LA. It will us that the complete solution to \(Ax = b\) is \(x = \text{ one } x_r + \text{ any } x_n \). Then the minimum norm solution to \(Ax = b\) is \(x = x_r\) from the row space plus \(x_n = 0\) from the nullspace, from \(\lVert x \rVert^2 = \lVert x_r \rVert^2 + \lVert  x_n \rVert^2   \)     

Every vector \(Ax\) is in the column space. Multiplying by \(A\) cannot do anything else. More importantly, every \(b\) in the column space comes from exactly one vector \(x_r\) in the row space.  

\textbf{Example 2: Every matrix of rank \(r\) has an \(r\) by \(r\) invertible submatrix. A has rank = 2:   }
\[
    \begin{bmatrix}
        1 & 2 & 3 & 4 & 5   \\
        1 & 2 & 4 & 5 & 6  \\
        1 & 2 & 4 & 5 & 6  \\
    \end{bmatrix}
    \text{ contains }
    \begin{bmatrix}
        1 & 3  \\
        1 & 4  \\
    \end{bmatrix}
    \text{ in the pivot rows and pivot columns.}
\] 

So if the submatrix \(C\) has \(r\) independent columns, then \(C\) and \(C^T\) has \(r\) independent columns. This locates an \(r\) by \(r\) invertible submatrix of \(A\). 

\subsection{Combing Bases from Subspaces}

Basis have two properties:
\begin{enumerate}
    \item They are linearly independent 
    \item They span the space
\end{enumerate}

However, the two properties implies eachother. \textbf{If there are \(n\) columns of independent vector in \(A\), then they span \(R^n\), \(Ax=b is solvable\). If \(n\) vectors span \(R^n\), they must be independent; \(Ax=b\) has one solution. If \(AB = I\) for square matrix, then \(BA = I\) too.   } 
We can also start from the opposite side, say \(Ax=b\) can be solved for every \(b\), forcing \textbf{existence of solution}. This means elimination produced no zero rows. There are \(n\) and not free variables. The nullspace contains only \(x=0\), ending in \textbf{uniqueness of solutions}. 

\textbf{Example 3:}
\[
    \text{For } A = 
    \begin{bmatrix}
        1 & 2  \\
        3 & 6  \\
    \end{bmatrix}
    \text{ split } x = 
    \begin{bmatrix}
         4 \\
         3 \\
    \end{bmatrix}
    \text{ into } x_r + x_n = 
    \begin{bmatrix}
         2 \\
         4 \\
    \end{bmatrix}
    + 
    \begin{bmatrix}
         2 \\
         -1 \\
    \end{bmatrix}
\] 

The vector (2, 4) is in the row space, while the orthogonal vector is from the nullspace (2, -1). 

\textbf{Example 4: Suppose \(S\) a six dimensional subspace of nine-dimensional space \(R^9\) }

\begin{enumerate}
    \item What are the possible dimensions of subspaces orthogonal to S? \textbf{0, 1, 2, 3 since \(S\) already took 6 of the total 9.}
    \item What are the possible dimension of the orthogonal complement \(S^{\perp}\) of \(S\)? \textbf{3, because they are asking for THE orthogonal complement, the one that contains everything else. So the biggest one, 3, is the answer. }
    \item What is the smallest size of matrix \(A\) that has row space \(S\)? \textbf{6 by 9, because A is a matrix of \(m \times n\) and given  \(R^n = R^9\), \(m \times 9\). For m, the dimension of the row space is the rank of m, so m = 6. Therefore, \(6 \ times 9\)  }
    \item What is the smallest possible size of a matrix \(B\) that has nullspace \(S^{\perp}\)? \textbf{6 by 9. We are told that \(R^9\) have an \(n\) of 9. And that \(S^{\perp} = 3\) from the second question, so we know that \(r + null = n \), we got \(r + 3 = 9\), r = 6. \(6 \times 9\)      }         
\end{enumerate} 

Notice how question 3 and 4 gives the same matrix? It is telling us that \(C(A^T) = (N(A))^{\perp} \) 

\subsection{Projection onto Lines and Subspaces}
\begin{enumerate}
    \item The projection of \(b\) onto the line through \(a\) is the closest point to \(b\): \(p = a(\frac{a^{T}b}{a^{T}a})\). 
    \item The error \(e = b - p\) is perpendicular to \(a\): the right triangle \(b p e\) has \(\lVert p \rVert^2 + \lVert e \rVert^2 = \lVert b \rVert^2   \). 
    \item The projection of \(b\) onto to asubspace \(S\) is the closest vector \(p\) in \(S\); \(b - p\) is orthogonal to \(S\). 
    \item \(A^{T}A\) is invertible and symmetric when \(A\) has independent columns: \(N(A^{T}A)= N(A)\).                 
    \item The projection of \(b\) onto \(C(A)\) is the vector \(p = A(A^{T}A)^{-1}A^{T}b\).
    \item The projection matrix onto \(C(A)\) is \(P = A(A^{T}A)^{-1}A^T\). It has \(p = Pb\) and \(P^2 = P = P^T\).  
\end{enumerate}

We can visual projection on a subspace S by having each vector \(b\) go the closest point \(p\) in \(S\). The error vector \(e = b - p\). If \(A\) has independent columns, the projection of \(b\) onto \(C(A)\) is \(p = A(A^{T}A^{-1})A^{T}b\). The projection matrix is \(P = A(A^{T}A)^{-1}A^T\). Its special property is \(P^2 = P\), a second projection changes nothing, because it projects onto itself. 

For some really nice subspaces, we can directly see their projection matrices. 
\begin{enumerate}
    \item What are the projections of \(b = (2, 3, 4)\) onto the \(z\) axis and the \(xy\) plane.
    \item What matrix \(P_1\) and \(P_2\) produce those projections onto a line and a plane?     
\end{enumerate}

When \(b\) is projected onto a line, its projection \(p\) is the part of the \(b\) along that line. If \(b\) is projected onto a plane, \(p\) is the part in that plane. The projection \(p\) is \(Pb\). 

We will call projection onto the \(z\) axis \(p_1\). Where projection \(p_2\) drop straight down to the \(xy\) plane. To with \(b = (2, 3, 4)\). \(p_1 = (0, 0, 4), p_2 = (2, 3, 0)\). 
We can see that 
\[
    \text{Onto the \(z\) axis: }
    P_1 = 
    \begin{bmatrix}
        0 & 0 & 0  \\
        0 & 0 & 0  \\
        0 & 0 & 1  \\
    \end{bmatrix}
    , \text{ Onto the \(xy\) plane: }
    P_2 = 
    \begin{bmatrix}
        1 & 0 & 0  \\
        0 & 1 & 0  \\
        0 & 0 & 0  \\
    \end{bmatrix}
\]      
\[
    p_1 = P_{1}b = 
    \begin{bmatrix}
         0 \\
         0 \\
         z \\
    \end{bmatrix},
    p_2 = P_{2}b = 
    \begin{bmatrix}
         x \\
         y \\
         0 \\
    \end{bmatrix}
\]

In this case, the projections \(p_1\) and \(p_2\) are perpendicular. The \(xy\) plane and the \(z\) axs are orthogonal subspaces. More than that, they are orthogonal complements; their dimensions add to \(1 + 2 = 3\). Every vector \(b\) in the whole space is the sum of its part in two subspaces. 
\begin{enumerate}
    \item The vectors give \(p_1 + p_2 = b\).
    \item The matrices give \(P_1 + P_2\) and \(P_{1}P_{2}=0\).   
\end{enumerate}

For this example, our goals have been reached, and it is the same for any line and any plane and any \(n\)-dimensional subspaces in \(m\) dimension. The objective is to find the part \(p\) in each subspaces, and the projection matrix \(P\) that produces that part \(p = Pb\). Every subspaces of \(R^m\) has its own \(m\) by \(m\) projection matrix \(P\). 

So, if the best description of a subspace is a basis, by putting the basis vectors into the columns \(A\), we are now projecting onto the column space of \(A\).We can say that the \(z\) axis is the column space of the \(3 \times 1 A_1\). The \(xy\) plane is the column space of \(A_2\). That \(xy\) plane too is also the column space of \(A_3\). So \(p_2 = p_3\) and \(P_2 = P_3\). 
\[
    A_1 = 
    \begin{bmatrix}
         0 \\
         0 \\
         1 \\
    \end{bmatrix}
    , A_2 = 
    \begin{bmatrix}
        1 & 0  \\
        0 & 1  \\
        0 & 0  \\
    \end{bmatrix}
    , A_3 = 
    \begin{bmatrix}
        1 & 2  \\
        2 & 3  \\
        0 & 0 \\
    \end{bmatrix}
\]          

Our problem is to project any \(b\) onto the column space of any \(m by n\) matrix. Starting with a line, dimension \(n = 1\), the matrix will have only one column; we will call it \(a\).    

\section{Projections onto Lines and Subspaces}

\section{Least Squares Approximations}

\section{Orthogonal Matrices and Gram-Schmidt}

\section{The Pseudoinverse of a Matrix}
