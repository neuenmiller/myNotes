\chapter{Orthogonality}

Two vectors are orthogonal (translated as right-angled from Greek) when their dot product is zero: \(v \cdot w = v^{T}w = 0\). The vectors in the two subspaces, the vectors in a basis, the column vectors in Q. Orthogonal vectors also have a curious behavior where it is also like pythagoras's theoreom. 
\[
    v^{T}w = 0 \text{ and } \left\lVert v \right\rVert^2 + \left\lVert w \right\rVert^2 = \left\lVert v + w \right\lVert^2
\] 

Important part, \textbf{the fundamental subspaces are orthogonal}
\begin{enumerate}
    \item N(A) contains all vectors orthogonal to the row space \(C(A^T)\).
    \item \(N(A^T)\)  contains all vectors orthogonal to the column space \(C(A)\). 
\end{enumerate} 
\(Ax = 0\) makes \(x\) orthogonal to each row, \(A^{T}y = 0\) make \(y\) orthogonal to each column. 

A key idea in this chapter is \textbf{projection}: If \(b\) is outside the column space of \(A\), then we must find the closest point \(p\) that is still inside. The line from \(b\) to \(p\) shows the error \(e\), and that line is perpendicular to the column space.    THe \textbf{least squares equation} \(A^{T}Ax = A^{T}b\) produces the closest \(p = Ax\) and smallest possible \(e\), it also gives the best \(e\) when \(Ax = b\) is unsolvable. The best \(x\) makes \(\left\lVert Ax - b \right\lVert\) as small as possible, the least squares. \(A^{T}Ax = A^{T}b\) is easy when \(A^{T}A = I\). Then \(A\) has orthonormal columns perpendicular unit vector. Remember that \(Q\) has \(Q^{T}Q = I\) and \(QR = A\), where the R is upper triangle. Orthogonal matrices are perfect for computations, \(A = QR\) is even better than \(A+LU\)               

\section{Orthogonality of Vectors and Subspaces}

\begin{enumerate}
    \item Orthogonal vectors have \(v^{T}w = 0\), then \(\left\lVert v \right\lVert^2\) + \(\left\lVert w \right\lVert\) = \(\left\lVert v + w \right\lVert^2\) as in \(a^2 + b^2 = c^2\)
    \item Subspace \(V\) and \(W\) are orthogonal when \(v^{T}w = 0\) for every \(v\) in \(V\) and every \(w\) in \(W\). 
    \item Row space of \(A\) is orthogonal to nullspace, column space of \(A\) is orthogonal to left nullspace.    
    \item The dimensions add to \(r + (n - r) = n\) and \(r + (m - r) = m\): orthogonal complements. 
    \item If \(n\) vectors in \(R^n\) are independent, they span \(R^n\). If \(n\) vectors span \(R^n\), they are independent.                 
\end{enumerate}

How can we proof that the entirety of the nullspace of \(A\) is orthogonal to the row space of \(A\)? Look at \(Ax = 0\). 
\[
    Ax = 
    \begin{bmatrix}
         \text{row 1 of \(A\) } \\
         \vdots \\
         \text{row $m$ of $A$} \\
    \end{bmatrix}
    \begin{bmatrix}
         x \\
    \end{bmatrix}
    =
    \begin{bmatrix}
         0 \\
         \vdots \\
         0 \\
    \end{bmatrix}
\]  
Notice how they are all zero? That means all of the row space is orthogonal to the nullspace. 

But given that we like to work with columns, another way you can proof it is: \( x^{T}(A^{T}y) = (Ax)^{T}y = 0^{T}y = 0 \)

\textbf{Importantly}, column space \(C(A)\) and left nullspace \(N(A^T)\) is also a perpendicular pair. The proof is more or less the same except we use \(A^T\) instead of \(A\). 

\textbf{Example 1:} The two rows of \(A\) are perpendicular to \(x\) in the nullspace of \(A\): 
\[
    Ax = 
    \begin{bmatrix}
        1 & -2 & 1  \\
        1 & 0 & -1  \\
    \end{bmatrix}
    \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         0 \\
         0 \\
    \end{bmatrix}
\]
or
\[
    A^{T}y = 
    \begin{bmatrix}
        1 & 1  \\
        -2 & 0 \\
        1 & -1  \\
    \end{bmatrix}
    \begin{bmatrix}
         0 \\
         0 \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         0 \\
         0 \\
         0 \\
    \end{bmatrix}
\]   

This is a fairly extreme case since \(C(A)\) is all of  \(R^2\) and the nullspace of \(A^T\) is a zero vector. The total dimension is \(2 + 1 = 3\). The subspaces accounted for all vectors in \(R^3 = R^n\). The column space and left nullspace have a dimension of \(2 + 0 = 2\), accounting for all vectors in \(R^{2} = R^m\). 

Note that \textbf{if V and W are orthogonal subspaces in \(R^{n} \) then \(\dim V + \dim W \leq n\)    }
Of course, imagine our house. We got a wall and a floor, they can't be orthogonal subspaces since they are both \(R^2\) and our world is \(R^3\), \(2 + 2 \geq 3\). So this is wrong. Some vector will lie in both the wall and the floor, which is the line where the wall meets the floor in both subspaces. 

Two orthogonal subspaces that account for the whole space have a special name, \textbf{orthogonal complements}. Orthogonal complement of \(V^{\perp}\) of \(V\) contains all vectors orthogonal to \(V\).    
So the two pairs of subspace in linear algebra are actually orthogonal complements. 
\begin{enumerate}
    \item Row space and null space, \(r + ( n - r) = n\)
    \item Column space and left nullspace \(r + (m - r)= m\)  
\end{enumerate}

Any vector \(x\) in \(R^n\) is the sum \(x = x_{\text{row}} + x_{\text{null}}\) of its row space and component and its null space component. Same goes for \(y\) in \(R^m\) is the sum \(y = y_{\text{col}} + y_{\text{null}}\), between its column space component and its component in \(N(A^T)\).

\textbf{Fundamental theoreom of Linear Algebra, Part 2:
\begin{enumerate}
    \item \(N(A)\) is the orthogonal complement of the row space \(C(A^T)\)  in \(R^n\)
    \item \(N(A^T)\) is the orthogonal complement of the column space \(C(A)\) in \(R^m\)    
\end{enumerate}} 


Check the figure on page 146 of Strang's LA. It will us that the complete solution to \(Ax = b\) is \(x = \text{ one } x_r + \text{ any } x_n \). Then the minimum norm solution to \(Ax = b\) is \(x = x_r\) from the row space plus \(x_n = 0\) from the nullspace, from \(\lVert x \rVert^2 = \lVert x_r \rVert^2 + \lVert  x_n \rVert^2   \)     

Every vector \(Ax\) is in the column space. Multiplying by \(A\) cannot do anything else. More importantly, every \(b\) in the column space comes from exactly one vector \(x_r\) in the row space.  

\textbf{Example 2: Every matrix of rank \(r\) has an \(r\) by \(r\) invertible submatrix. A has rank = 2:   }
\[
    \begin{bmatrix}
        1 & 2 & 3 & 4 & 5   \\
        1 & 2 & 4 & 5 & 6  \\
        1 & 2 & 4 & 5 & 6  \\
    \end{bmatrix}
    \text{ contains }
    \begin{bmatrix}
        1 & 3  \\
        1 & 4  \\
    \end{bmatrix}
    \text{ in the pivot rows and pivot columns.}
\] 

So if the submatrix \(C\) has \(r\) independent columns, then \(C\) and \(C^T\) has \(r\) independent columns. This locates an \(r\) by \(r\) invertible submatrix of \(A\). 

\subsection{Combing Bases from Subspaces}

Basis have two properties:
\begin{enumerate}
    \item They are linearly independent 
    \item They span the space
\end{enumerate}

However, the two properties implies eachother. \textbf{If there are \(n\) columns of independent vector in \(A\), then they span \(R^n\), \(Ax=b is solvable\). If \(n\) vectors span \(R^n\), they must be independent; \(Ax=b\) has one solution. If \(AB = I\) for square matrix, then \(BA = I\) too.   } 
We can also start from the opposite side, say \(Ax=b\) can be solved for every \(b\), forcing \textbf{existence of solution}. This means elimination produced no zero rows. There are \(n\) and not free variables. The nullspace contains only \(x=0\), ending in \textbf{uniqueness of solutions}. 

\textbf{Example 3:}
\[
    \text{For } A = 
    \begin{bmatrix}
        1 & 2  \\
        3 & 6  \\
    \end{bmatrix}
    \text{ split } x = 
    \begin{bmatrix}
         4 \\
         3 \\
    \end{bmatrix}
    \text{ into } x_r + x_n = 
    \begin{bmatrix}
         2 \\
         4 \\
    \end{bmatrix}
    + 
    \begin{bmatrix}
         2 \\
         -1 \\
    \end{bmatrix}
\] 

The vector (2, 4) is in the row space, while the orthogonal vector is from the nullspace (2, -1). 

\textbf{Example 4: Suppose \(S\) a six dimensional subspace of nine-dimensional space \(R^9\) }

\begin{enumerate}
    \item What are the possible dimensions of subspaces orthogonal to S? \textbf{0, 1, 2, 3 since \(S\) already took 6 of the total 9.}
    \item What are the possible dimension of the orthogonal complement \(S^{\perp}\) of \(S\)? \textbf{3, because they are asking for THE orthogonal complement, the one that contains everything else. So the biggest one, 3, is the answer. }
    \item What is the smallest size of matrix \(A\) that has row space \(S\)? \textbf{6 by 9, because A is a matrix of \(m \times n\) and given  \(R^n = R^9\), \(m \times 9\). For m, the dimension of the row space is the rank of m, so m = 6. Therefore, \(6 \ times 9\)  }
    \item What is the smallest possible size of a matrix \(B\) that has nullspace \(S^{\perp}\)? \textbf{6 by 9. We are told that \(R^9\) have an \(n\) of 9. And that \(S^{\perp} = 3\) from the second question, so we know that \(r + null = n \), we got \(r + 3 = 9\), r = 6. \(6 \times 9\)      }         
\end{enumerate} 

Notice how question 3 and 4 gives the same matrix? It is telling us that \(C(A^T) = (N(A))^{\perp} \)  


\section{Projections onto Lines and Subspaces}

\subsection{Projection onto Lines and Subspaces}
\begin{enumerate}
    \item The projection of \(b\) onto the line through \(a\) is the closest point to \(b\): \(p = a(\frac{a^{T}b}{a^{T}a})\). 
    \item The error \(e = b - p\) is perpendicular to \(a\): the right triangle \(b p e\) has \(\lVert p \rVert^2 + \lVert e \rVert^2 = \lVert b \rVert^2   \). 
    \item The projection of \(b\) onto to asubspace \(S\) is the closest vector \(p\) in \(S\); \(b - p\) is orthogonal to \(S\). 
    \item \(A^{T}A\) is invertible and symmetric when \(A\) has independent columns: \(N(A^{T}A)= N(A)\).                 
    \item The projection of \(b\) onto \(C(A)\) is the vector \(p = A(A^{T}A)^{-1}A^{T}b\).
    \item The projection matrix onto \(C(A)\) is \(P = A(A^{T}A)^{-1}A^T\). It has \(p = Pb\) and \(P^2 = P = P^T\).  
\end{enumerate}

We can visual projection on a subspace S by having each vector \(b\) go the closest point \(p\) in \(S\). The error vector \(e = b - p\). If \(A\) has independent columns, the projection of \(b\) onto \(C(A)\) is \(p = A(A^{T}A^{-1})A^{T}b\). The projection matrix is \(P = A(A^{T}A)^{-1}A^T\). Its special property is \(P^2 = P\), a second projection changes nothing, because it projects onto itself. 

For some really nice subspaces, we can directly see their projection matrices. 
\begin{enumerate}
    \item What are the projections of \(b = (2, 3, 4)\) onto the \(z\) axis and the \(xy\) plane.
    \item What matrix \(P_1\) and \(P_2\) produce those projections onto a line and a plane?     
\end{enumerate}

When \(b\) is projected onto a line, its projection \(p\) is the part of the \(b\) along that line. If \(b\) is projected onto a plane, \(p\) is the part in that plane. The projection \(p\) is \(Pb\). 

We will call projection onto the \(z\) axis \(p_1\). Where projection \(p_2\) drop straight down to the \(xy\) plane. To with \(b = (2, 3, 4)\). \(p_1 = (0, 0, 4), p_2 = (2, 3, 0)\). 
We can see that 
\[
    \text{Onto the \(z\) axis: }
    P_1 = 
    \begin{bmatrix}
        0 & 0 & 0  \\
        0 & 0 & 0  \\
        0 & 0 & 1  \\
    \end{bmatrix}
    , \text{ Onto the \(xy\) plane: }
    P_2 = 
    \begin{bmatrix}
        1 & 0 & 0  \\
        0 & 1 & 0  \\
        0 & 0 & 0  \\
    \end{bmatrix}
\]      
\[
    p_1 = P_{1}b = 
    \begin{bmatrix}
         0 \\
         0 \\
         z \\
    \end{bmatrix},
    p_2 = P_{2}b = 
    \begin{bmatrix}
         x \\
         y \\
         0 \\
    \end{bmatrix}
\]

In this case, the projections \(p_1\) and \(p_2\) are perpendicular. The \(xy\) plane and the \(z\) axs are orthogonal subspaces. More than that, they are orthogonal complements; their dimensions add to \(1 + 2 = 3\). Every vector \(b\) in the whole space is the sum of its part in two subspaces. 
\begin{enumerate}
    \item The vectors give \(p_1 + p_2 = b\).
    \item The matrices give \(P_1 + P_2\) and \(P_{1}P_{2}=0\).   
\end{enumerate}

For this example, our goals have been reached, and it is the same for any line and any plane and any \(n\)-dimensional subspaces in \(m\) dimension. The objective is to find the part \(p\) in each subspaces, and the projection matrix \(P\) that produces that part \(p = Pb\). Every subspaces of \(R^m\) has its own \(m\) by \(m\) projection matrix \(P\). 

So, if the best description of a subspace is a basis, by putting the basis vectors into the columns \(A\), we are now projecting onto the column space of \(A\).We can say that the \(z\) axis is the column space of the \(3 \times 1 A_1\). The \(xy\) plane is the column space of \(A_2\). That \(xy\) plane too is also the column space of \(A_3\). So \(p_2 = p_3\) and \(P_2 = P_3\). 
\[
    A_1 = 
    \begin{bmatrix}
         0 \\
         0 \\
         1 \\
    \end{bmatrix}
    , A_2 = 
    \begin{bmatrix}
        1 & 0  \\
        0 & 1  \\
        0 & 0  \\
    \end{bmatrix}
    , A_3 = 
    \begin{bmatrix}
        1 & 2  \\
        2 & 3  \\
        0 & 0 \\
    \end{bmatrix}
\]          

Our problem is to project any \(b\) onto the column space of any \(m by n\) matrix. Starting with a line, dimension \(n = 1\), the matrix will have only one column; we will call it \(a\). 

\subsection{Projection Onto a Line}

The thing we must remember is how the line from \(b\) to \(p\) called \(e\) is always perpendicular to \(a\). The projection \(p\) will be some multiple of \(a\), calling it \(p = \hat{x}a\). So first we need to find \(\hat{x}\), then vector \(p\), then matrix \(P\). Oh, and now \(e = b - \hat{x}a \) too. 
\[
    \hat{x} = \frac{a \cdot b}{a \cdot a} = \frac{a^{T}b}{a^{T}a}
\]          
The tranpose version is better because we can use it with matrices too. 
\[
    \text{For line, }
    e = b - p 
    p = \hat{x}a = \frac{a^{T}b}{a^{T}a}a
\] 
\[
    \text{For plane project, }
    p = A\hat{x} = A(A(A^{T}A)^{-1}A^{T}b) = Pb
\]

\textbf{Example one:} Find the \(p = \hat{x}a\)
\[
    b = 
    \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
    \text{ onto }
    a = 
    \begin{bmatrix}
         1 \\
         2 \\
         2 \\
    \end{bmatrix}
\]  
\(\hat{x}\) is a ratio between \(\frac{a^{T}b}{a^{T}a} = \frac{5}{9}\). So \(p = \frac{5}{9}a = (\frac{5}{9}, \frac{10}{9}, \frac{10}{9})\) and \(e = b - p = (\frac{4}{9}, -\frac{1}{9}, \frac{1}{9})\). 
And to check that \(e\) is perpendicular to \(a = (1, 2, 2)\), which it is: \(e^{T}a = \frac{4}{9} - \frac{2}{9} - \frac{2}{9}= 0 \). 

So, about the projection matrix. In the formula for \(p\), what matrix is the multiplying \(b\)?
\[
    p = a\hat{x} = a\frac{a^{T}b}{a^{T}a} = Pb
    \text{ when }
    P = \frac{aa^T}{a^{T}a}
\]  
We can see that it is a column timed a row, where \(a\) is a column and \(a^T\) is a row. The projection matrix \(P\) is \(m \times m\) but its rank is \textbf{one} . We are just projecting onto a one-dimensional subspace, the line through \(a\). That line is the column space of \(P\). This proves that \(p\) will always be on \(a\). 

\textbf{Example 2:} Find the projection matrix \(P = \frac{aa^T}{a^{T}a}\) onto the line \(a = \begin{bmatrix}
     1 \\
     2 \\
     2 \\
\end{bmatrix}\). 
\[
    \text{Projection matrix}
    P = \frac{aa^T}{a^{T}a} = \frac{1}{9}
    \begin{bmatrix}
         1 \\
         2 \\
         2 \\
    \end{bmatrix}
    \begin{bmatrix}
        1 & 2 & 2  \\
    \end{bmatrix}
    = \frac{1}{9}
    \begin{bmatrix}
        1 & 2 & 2  \\
        2 & 4 & 4  \\
        2 & 4 & 4  \\
    \end{bmatrix}
\]   

This matrix projects \textbf{any} vector \(b\) onto \(a\). Check \(p = Pb\) for \(b = (1, 1, 1)\) in example 1: 
\[
    p = Pb = \frac{1}{9}
    \begin{bmatrix}
        1 & 2 & 2  \\
        2 & 4 & 4  \\
        2 & 4 & 4  \\
    \end{bmatrix}
    \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
    = 
    \frac{1}{9}
    \begin{bmatrix}
         5 \\
         10 \\
         10 \\
    \end{bmatrix}
\]    
\[
    e = b - p = \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
    - \frac{1}{9}
    \begin{bmatrix}
         5 \\
         10 \\
         10 \\
    \end{bmatrix}
    = 
    \frac{1}{9}
    \begin{bmatrix}
         4 \\
         -1 \\
         -1 \\
    \end{bmatrix}
\]

Even if vector \(a\) is doubled, the \(P\) matrix stays the same. Again, for reminder, projecting a second line doesn't change anything since it is already at the closest spot. 

Another thing could be a projection, \(I - P\), which projects onto the perpendicular subspace, ie a plane perpendicular to \(a\), the orthogonal complement.

\subsection{Projection Onto a Subspace}

Start with \(n\) vectors \(a_1, \ldots, a_n\) in \(R^m\). Assume the \(a\)'s are linearly independent. 
Now, \textbf{find the combination \(p = \hat{x_1}a_1 + \ldots + \hat{x_n}a_n\) closest to a given vector b}. We are project each \(b\) in \(R^m\) onto the \(n\)-dimensional subspace spanned by the \(a\)'s. Where with just \(n = 1\), it's just a line, which is a column space of \(A\); where it is just one column. You should know that the hat on \(\hat{x}\) means the best. So when we say we are looking for particular solution \(p = A\hat{x}\), we are saying we are looking for the best approximation to \(b\) . And for what we know, \(\hat{x} = \frac{a^{T}b}{a^{T}a}\) when \(n = 1\). For the next part, we will try to find \(\hat{x}\)  when \(n > 1\). 

We can compute projections onto \(n\)-dimensions subspaces in three steps as before:
\begin{enumerate}
    \item Find the vector \(\hat{x}\) in \((S)\). 
    \item Find the projection \(p = A\hat{x}\) in \((C)\) .
    \item Find the projection matrix \(P\) in \((T)\). 
\end{enumerate} 

Remember that the error vector \(b - A\hat{x}\) is perpendicular to the subspace, where it makes right angle with all the vector in the subspace. 
\[
    \begin{bmatrix}
         a^T_1 \\
         \ldots \\
         a^T_n \\
    \end{bmatrix}
    \begin{bmatrix}
        b - A\hat{x} \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         0 \\
    \end{bmatrix}
    \text{ or }
    A^{T}A\hat{x} = A^{T}b
\] 

We can see that \(A^{T}(b-A\hat{x})\), which can be rewritten was \(A^{T}A\hat{x} = A^{T}b\). This is the equation for \(\hat{x}\), where we can see \(A^{T}A\) as a coefficient matrix. With this, we can now find \(\hat{x}, p,\) and \(P\) in that order. \textbf{REMEMBER THE NEXT THREE EQUATIONS!}
\begin{enumerate}
    \item Find \(\hat{x} (n \times 1)\) \textbf{\(A^{T}(b - A\hat{x} = 0)\) or \(A^{T}A\hat{x} = A^{T}b\)} . 
    \item The symmetric matrix \(A^{T}A\) is \(n \times n\). It is invertible if the \(a\)'s are independent. The solution is \(\hat{x} = (A^{T}A)^{-1}A^{T}b\) we can find the projection of b onto the subspace is p: \textbf{Find \(p (m \times 1) p = A\hat{x} = A(A^{T}A)^{-1}A^{T}b \)}. 
    \item The projection matrix \(P\) is multiplying \(b\) in the last example, the one with four \(A\)'s. \textbf{Find \(P (m \times m)\), \(P = A(A^T A)^-1 A^T\)  }       
\end{enumerate} 

For \(n = 1\)
\begin{enumerate}
    \item \(\hat{x} = \frac{a^T b}{a^T}a\)
    \item \(p = a \frac{a^T b}{a^T a}\)
    \item \(P = \frac{a a^T}{a^T a}\)   
\end{enumerate} 

It is very similar to the prior 3 equations. Where the number \(a^T a\) becomes \(A^T A\). When it is a number, we divide by it, when it becomes a matrix, we invert it. Since \(a_1, \ldots, a_n\) is independent, that means \(A^T A\) is invertible. From \(A^T (b - A\hat{x})\), we can tell that \(e\) is orthogonal to each \(a\). 
\begin{enumerate}
    \item Our subspace \(C(A)\) is the column space of \(A\)
    \item The error \(e = b - A\hat{x}\) is in the perpendicular subspace \(N(A^T)\)    
\end{enumerate}       
That means that \(A^T (b - A\hat{x}) = 0\) and that left nullspace \(N(A^T)\) is important in projection since it contains the error vector \(e = b - A\hat{x}\). The vector \(b\) is split into the projection \(p\) and the error \(e\), where the production produces a right triangle with sides \(p, e, b\), remember we are talking about vector, the line begin from origin. 

\textbf{Example 3:} If \(A = \begin{bmatrix}
    1 & 0  \\
    1 & 1  \\
    1 & 2  \\
\end{bmatrix}\) and \(b = \begin{bmatrix}
     6 \\
     0 \\
     0 \\
\end{bmatrix}\), find \(\hat{x}, p, P\)
\begin{enumerate}
    \item Find \(A^T A\) and \(A^T b\) 
    \item Solve the normal equation \(A^T A\hat{x} = A^T b\) to find \(\hat{x}\)
    \item The combination \(p = A\hat{x}\) is the projection of \(b\) onto the column space of \(A\).       
\end{enumerate}   
\[
    A^T A =
    \begin{bmatrix}
        1 & 1 & 1  \\
        0 & 1 & 2  \\
    \end{bmatrix}
    \begin{bmatrix}
        1 & 0  \\
        1 & 1  \\
        1 & 2   \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        3 & 3  \\
        3 & 5  \\
    \end{bmatrix}
    \text{ and }
    A^T b = 
    \begin{bmatrix}
        1 & 1 & 1  \\
        0 & 1 & 2  \\
    \end{bmatrix}
    \begin{bmatrix}
         6 \\
         0 \\
         0 \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         6 \\
         0 \\
    \end{bmatrix}
\]

\[
    \begin{bmatrix}
        3 & 3  \\
        3 & 5  \\
    \end{bmatrix}
    \begin{bmatrix}
         \hat{x_1} \\
         \hat{x_2} \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        6  \\
        0  \\
    \end{bmatrix}
    \text{ gives }
    \hat{x}
    = 
    \begin{bmatrix}
         \hat{x_1} \\
         \hat{x_2} \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         5 \\
         -3 \\
    \end{bmatrix}
\]
\[
    p = 
    5 \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
    - 3 
    \begin{bmatrix}
         0 \\
         1 \\
         2 \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         5 \\
         2 \\
         -1 \\
    \end{bmatrix},
    \text{ The error is }
    e = b - p = 
    \begin{bmatrix}
         1 \\
         -2 \\
         1 \\
    \end{bmatrix}
\]

We can now try to find \(P\) matrix. The projection is \(P = A(A^T A)^{-1} A^T \). The determinant of \(A^T A\) is \(15 - 9 = 6\), then also inverse the \(2 \times 2\). Multiple \(A \times (A^T A)^{-1} \times A = P \)  
\[
    (A^T A)^{-1} = 
    \frac{1}{6}
    \begin{bmatrix}
        5 & -3  \\
        -3 & 3  \\
    \end{bmatrix}
    \text{ and }
    P = \frac{1}{6}
    \begin{bmatrix}
        5 & 2 & -1  \\
        2 & 2 & 2  \\
        -1 & 2 & 5  \\
    \end{bmatrix} 
\]    
And \(P^2 = P\) must be true because we discuss before, it projects onto itself. 

\textbf{Warning!} \(P = A(A^T A)^{-1} A^T \) can be deceptive. We can't split \((A^T A)^{-1} \) into \(A^{-1} \times (A^T)^{-1}  \). If we do that, we will find that \(P = AA^{-1}(A^T)^{-1} A^T  \) collapses into \(P = I\), which means this is wrong. Why? \textbf{Matrix A is rectangular, which means it has no inverse matrix (as in, \(A^{-1} \) )}. We can split \((A^T A)^{-1} \) into \(A^{-1} \times (A^T)^{-1} \) when \(m > n\). 
Here is something we need to remember: 
\[
    \text{\(A^T A\) is invertible if and only if A has full-rank column.}
\]          
\[
    \text{When \(A\) has independent columns, \(A^T A\) is square, symmetric, and invertible.}
\]
For emphasis, \(A^T A\) is \(n \times m\) times \(m \times n\), \(A^T A\) is square \(n \times n\). We just again prove \(A^T A\) is invertible, provided A is full-rank column. 
\[
    \begin{bmatrix}
        1 & 1 & 0  \\
        2 & 2 & 0  \\
    \end{bmatrix}
    \begin{bmatrix}
        1 & 2  \\
        1 & 2  \\
        0 & 0  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        2 & 4   \\
        4 & 8  \\
    \end{bmatrix}
\]      
\[
    \begin{bmatrix}
        1 & 1 & 0  \\
        2 & 2 & 1  \\
    \end{bmatrix}
    \begin{bmatrix}
        1 & 2  \\
        1 & 2  \\
        0 & 1  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        2 & 4  \\
        4 & 9  \\
    \end{bmatrix}
\]
The first example has dependent \(A\) and singular output, where the second example has full-rank columns and the output is invertible. 

\textbf{Very brief summary,} to find \(p = \text{x}_1 a_1 + \ldots + \hat{x}_n a_n\), solve \(A^T A\hat{x} = A^T b\). This gives us \(\hat{x}\). The projection is \(p = A\text{x}\) and the error is \(e = b - p = b - A\text{x}\). The projection matrix gives \(P = A(A^T A)^{-1} A^T \) gives \(p = Pb\). \(P\) is invertible only if \(P = I\). 
This matrix satisfies \(P^2 = P\) and the distance from \(b\) to subspace \(C(A)\) is \(\lVert e \rVert \).

Here is a review of a few key ideas, 
\begin{enumerate}
    \item Projection of \(b\) onto \(a\) is \(p = a\hat{x} = a(\frac{a^T b}{a^T a})\). 
    \item Rank one projection is \(P = \frac{aa^T}{a^T a}\) multiples with \(b\) to produce \(p\). 
    \item Projecting \(b\) onto a subspace leaves \(e = b - p\) perpendicular to the subspace.
    \item When \(A\) has full rank \(n\), the equation \(A^T A \hat{x} = A^T b\) leads to \(\hat{x}\) and \(p = A\hat{x}\). 
    \item Projection matrix \(P = A(A^T A)^{-1} A^T \) has \(P^T = P\) and \(P^2 = P\) and \(Pb = p\)                 
\end{enumerate}
\section{Least Squares Approximations}

A lot of \(Ax = b\) has no solution due to there being \emph{too} many equations, which happens when there is more rows than columns \((m > n)\). The \(n\) columns spans only a \textbf{small} part of the \emph{\(m\)-dimensional space } 
\[
    \text{When \(Ax = b\) has no solution, multiply by \(A^T\) and solve \(A^T A \hat{x} = A^T b\)   }
\]  

\textbf{Example 1:} One of the biggest use for least square is fitting a straight line to \(m\) points. 
Say we have 3 points, we need to find the line closest to them \((0,6), (1, 0), (2, 0)\)
No straight line go through these points, but we can turn them into three equations \((n = 2, m = 3)\). 
\begin{enumerate}
    \item \(t = 0\), the first point is on the line \(b = C + Dt\) if \(C \cdot D = 6\) 
    \item \(t = 1\), the second point is on the line \(b = C + Dt\) if \(C \cdot D = 0\) 
    \item \(t = 2\), the third point is on the line \(b = C + Dt\) if \(C \cdot D = 0\) 
\end{enumerate}  

\[
    A = 
    \begin{bmatrix}
        1 & 0  \\
        1 & 1  \\
        1 & 2  \\
    \end{bmatrix}, 
    x = 
    \begin{bmatrix}
         C \\
         D \\
    \end{bmatrix}, 
    b = 
    \begin{bmatrix}
         6 \\
         0 \\
         0 \\
    \end{bmatrix}
    , \text{ Ax = b is not solvable}
\]

This is the same question as example 3 in the last section, we computed that \(\hat{x} = (5, -3)\). So the line \(5 -3t\) will be the best for the three points. 

\subsection{Minimizing the error}

How do we make \(e = b - Ax\) as small as possible is a very important question. \(\hat{x}\) can be fonund by geometry, where \(e\) is perpendicular to \(A\). Calculus also gives the same \(\hat{x}\), the derivative of the error \(\lVert Ax -  \rVert^2 \) is zero \(\hat{x}\). 

\emph{By geometry}, every \(Ax\) lies in the plane of the column \((1, 1, 1)\) and \((0, 1, 2)\). In that plane, we will need to look for the closest point to \(b\), which is \(p\). While all three points can't be connected with one line, they are all still within the column space of \(A\). Therefore, in a fitting straight line, \(\hat{x} = (C, D)\) is the best choice. 

\emph{By algebra}, \(b\) splits into two parts, the part in column space is \(p\), the perpendicular part is \(e\). We can not solve \(Ax = b\), but we can solve \(A\hat{x} = p\) (by removing \(e\) and soling \(A^T A\hat{x} = A^T b\)  )
\[
    Ax = b = p + e 
    \text{ is impossible}
\]     
\[
    A\hat{x} = p 
    \text{ is solvable}
\]
\[
    \hat{x} = (A^T A)^{-1} A^T b  
\]

The solution to \(A\hat{x} = p\) leaves the least possible error, \(e\): 
\[
    \text{ Squared error for any \(x\), }
    \lVert Ax - b \rVert^2 = \lVert Ax - p \rVert^2 + \lVert e \rVert^2   
\]  

We reduce \(Ax - p\) to zero by choosing \(x = \hat{x}\), so we get the smallest \(e = (e_1, e_2, e_3)\), which we can't reduce further. 
\[
    \text{The least square solutions \(\hat{x}\) makes } E = \lVert Ax - b \rVert^2 \text{ as small as possible.} 
\]   

Look along at figure 4.6 on page 165. We can see that the best line is \(b = 5 - 3t\), and the closest point is \(p = 5a_1 - 3a_2\). 

Notice how the errors are \((1, -2, 1)\), adding to zero. The reason is that the error \(e = (e_1, e_2, e_3)\) is perpendicular to the first column (and the second column too) \((1, 1, 1)\) in \(A\). The dot product gives \(e_1 + e_2 + e_3 = 0\). 
\[
    e = \begin{bmatrix}
         e_1 \\
         e_2 \\
         e_3 \\
    \end{bmatrix}
    \rightarrow
    e \cdot
    \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
    = 
    e_1 + e_2 + e_3 
    = 
    0
\]

Most functions are minimized by calculus, where the graph bottoms out and the derivative is every direction is \textbf{zero} . Where \(E\)is minimized is a sum of squares. 
\[
    E = \lVert Ax - b \rVert^2 = (C + D \cdot 0 - 6)^2 + (C + D \cdot 1)^2 + (C + D \cdot 2)^2 
\] 

Since there are two unknowns, where we want the two derivative at zero, we will need to do partial derivative. 
\[
    \frac{\partial E}{\partial C} = 2(C + D \cdot 0 - 6)^2 + 2(C + D \cdot 1)^2 + 2(C + D \cdot 2)^2 = 0
\]
\[
    \frac{\partial E}{\partial D} = 2(C + D \cdot 0 - 6)^2 (0) + 2(C + D \cdot 1)^2 (1) + 2(C + D \cdot 2)^2 (2) = 0
\]

\(\frac{\partial E}{\partial D}\) contains factor \emph{0, 1, 2} from the chain rule. It is not an accidental that the factor \((1, ,1 ,1)\) and \((0,1,2)\) is in the derivative of \(\lVert Ax - b \rVert^2 \) are in the columns of \(A\). Now cancel 2 from every term and we get: 
\begin{enumerate}
    \item C derivative is zero: \(3C + 3D = 6\)
    \item D derivative is zero: \(3C + 5D = 0\) 
\end{enumerate}     
\[
    \begin{bmatrix}
        3 & 3  \\
        3 & 5  \\
    \end{bmatrix}
    \text{ is \(A^T A\) }
\]

Those two equations are \(A^T A \hat{x} = A^T b\). The best \(C\) and \(D\) are components of \(\hat{x}\). The equations from calculus are the same as the normal equations from linear algebra. These equation are to minimize \(\lVert Ax - b \rVert^2 = x^T A^T Ax - 2x^T A^T b + b^T b \)
\[
    \text{The partial derivatives of  } \lVert Ax - b \rVert^2 \text{ are zero when } A^T A\hat{x} = A^T b  
\]     

And the solution are \(C = 5\)and \(D = -3\) therefore \(b = 5 - 3t\) is the best line. This is the vector of \(e\). 

\subsection{The Big Picture for Least Square}

This correspond to page 166 (176), figure 4.7. We can remember  how on an older "big picture" figure, that we needed to split \(x\) into \(x_p + x_n\). That was for when there were \textbf{many} solutions. This is for cases where there is \textbf{no} answer to \(Ax = b\), however. Instead of splitting \(x\) we are splitting \(b = p + e\). Instead of \(Ax = b\), we solve \(A\hat{x} = p\). 
From the figure 4.7, we can see that the nullspace \(N(A)\) is very small. Remember that \(A^T A\) is an invertible matrix. The equation \(A^T A \hat{x} = A^T b\) determines \(\hat{x}\). The error \(e = b - p\) has \(A^T e = 0\). 

\subsection{Fitting a Straight Line}

Fitting a line requires \(m > 2\) points; at time \(t_1, \ldots, t_2\), those points are at height \(b_1, \ldots, b_m\). The best line \(C + Dt\) misses the point by vertical distance \(e_1, \ldots, e_m\). No line is perfect, so we want to minimize \(E = e^2_1 + \ldots + e^2_m\). We used \(m = 3\) for the last example, but really, \(m\) can be a very large number; though we will still have the component of \(\hat{x}\)  be \(C\) and \(D\).
When \(Ax = b\) is solved exactly, the line goes through the \(m\) points, which rarely happens. Two unknown \((C, D)\) determines a line, so \(A\) has \(n = 2\) columns. 
\[
    Ax = b 
    \text{ is }
    \begin{bmatrix}
         C + Dt_1 = b_1 \\
         C + Dt_2 = b_2\\
         \vdots \\
         C + Dt_m = b_m \\
    \end{bmatrix}
    \text{ with }
    A = 
    \begin{bmatrix}
        1 & t_1  \\
        1 & t_2  \\
        \vdots & \vdots  \\
        1 & t_m  \\
    \end{bmatrix}
\]     

The column space is so so thin that it is almost certain that \(b\) is outside of it. When \(b\) \textbf{does} lie inside of it, that means the points are on the line. In that case, \(b = p\), therefore \(Ax = b\) and \(e = (0, \ldots, 0)\). 

The closest line \(C + Dt\) has heights \(p_1, \ldots, p_m\) with errors \(e_1, \ldots, e_m\). 

Solve \(A^T A = A^T b\) for \( \hat{x} = (C, D)\). The errors are \(e_i = b_i - C -Dt_i\). 
\[
    \text{Dot-product matrix}
    A^T A = 
    \begin{bmatrix}
        1 & \ldots & 1  \\
        t_1 & \ldots & t_m  \\
    \end{bmatrix}
    \begin{bmatrix}
        1 & t_1  \\
        \ldots & \ldots  \\
        1 &  t_m \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        m & \Sigma t_i  \\
        \Sigma t_i & \Sigma t^2_i  \\
    \end{bmatrix}
\]

On the right side of the normal equation is the \(2 \times 1\) vector \(A^T b\):
\[
    A^T A 
    \begin{bmatrix}
         C \\
         D \\
    \end{bmatrix}
    = A^T A\hat{x} = A^T b = 
    \begin{bmatrix}
        1 & \ldots & 1  \\
        t_1 & \ldots & t_m  \\
    \end{bmatrix}
    \begin{bmatrix}
         b_1 \\
         \ldots \\
         b_m \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         \Sigma b_i \\
         \Sigma t_i b_i \\
    \end{bmatrix}
    \]  

The best  \(\hat{x} = (C, D)\) is \((A^T A)^{-1} A^T b \): 
\[
    A^T A\hat{x} = A^T b 
    \begin{bmatrix}
        m & \Sigma t_i  \\
        \Sigma t_i & \Sigma t^2_i  \\
    \end{bmatrix}
    \begin{bmatrix}
         C \\
         D \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         \Sigma b_i \\
         \Sigma t_i b_i \\
    \end{bmatrix}
\]  

The vertical errors at the \(m\) points on the line are the components of the \(e = b - p\). \emph{The error vector is perpendicular to the column of \(A\), in term of geometry. The error is in the nullspace of \(A^T\), in term of linear algebra. The best \(\hat{x} = (C, D)\) minimizes the total error \(E\), the sum of squares, in term of calculus    }
\[
    E(x) = \lVert Ax - b \rVert^2 = (C + Dt_1 - b_1)^2 + \ldots + (C + Dt_m - b_m)^2 
\]
Calculus sets the derivative \(\frac{\partial E}{\partial C}\) and \(\frac{\partial E}{\partial C}\) to zero, and recovers \(A^T A\hat{x} = A^T b\). A lot of other problem have more than two unknowns. In general, we are fitting \(m\) data points by \(n\) parameters \(x_1, \ldots, x_n\). The matrix \(A\). The matrix \(A\) has \(n\) columns and \(n < m\). The derivative of \(\lVert Ax - b \rVert^2 \) gives the \(n\) equations \(A^T A \hat{x} = A^T b\), the derivative of a square is linear. 

\textbf{Example 2:} \(A\) has an \textbf{orthogonal} columns when the measurement times \(t_i\) add to zero. Suppose \(b = 1, 2, 4\) when \(t = -2, 0, 2\), where the time adds to zero. The columns of \(A\) have zero dot product: \((1, 1, 1)\) is orthogonal to \(02, 0, 2\):
\[
    \begin{bmatrix}
         C + D(-2) = 1\\
         C + D(0) = 2\\
         C + D(2) = 4\\
    \end{bmatrix}
    \text{ or }
    Ax = 
    \begin{bmatrix}
        1 & -2  \\
        1 & 0  \\
        1 & 2  \\
    \end{bmatrix}
    \begin{bmatrix}
         C \\
         D \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         7 \\
         6 \\
    \end{bmatrix}
\]       
Since \(A^T A\) is diagonal, we can easily solve separate that \(C = \frac{7}{3}\) and \(D = \frac{6}{8}\). The zeros in \(A^T A\) are dot products of perpendicular columns in \(A\). With the lower right input, we can see that \(t^2_1 + t^2_2 + t^2_3 = 8\). 

Orthogonal columns are very helpful so much so that it can be worth shifting the times by subtracting the average time \(\hat{t}\). If the average times were 1, 3, 5 then their average is \(\hat{t} = 3\). The shifted times \(T = t - \hat{t} = t - 3\) adds up to zero. 
\[
    \begin{bmatrix}
         T_1 = 1 - 3 = 2 \\
         T_2 = 3 - 3 = 0 \\
         T = 5 - 3 = 2 \\
    \end{bmatrix}
    , 
    A_{new} = 
    \begin{bmatrix}
        1 & T_1  \\
        1 & T_2  \\
        1 & T_3  \\
    \end{bmatrix} 
    , 
    A^T_{new} A = 
    \begin{bmatrix}
        3 &  0 \\
        0 &  8 \\
    \end{bmatrix} 
\]   

\subsection{Dependent Columns in A: What is \(\hat{x}\) ?}

So far, we have assumed that the columns in \(A\) are independent. What if they aren't? For this example: 
\[
    \begin{bmatrix}
        1 & 1  \\
        1 & 1  \\
    \end{bmatrix}
    \begin{bmatrix}
         x_1 \\
         x_2 \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         3 \\
         1 \\
    \end{bmatrix}
    = b 
    \text{, Ax = b}
\] 
\[
    \begin{bmatrix}
        1 & 1  \\
        1 & 1  \\
    \end{bmatrix}
    \begin{bmatrix}
         \hat{x_1} \\
         \hat{x_2} \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         2 \\
         2 \\
    \end{bmatrix}
    = p
    \text{, Ax = b}
\] 

We can see a figure on page 169 (179), at the same \(T\), \(b_1 = 3\) and \(b_2 = 1\). A straight line \(C + Dt\) can't go through both points. If we can first try to see the column space of \(A\) but it still provides us with 3 solutions. We could do \(\hat{x}_1 = C = 2, \hat{x}_2 = D = 0\). We could also do \(\hat{x}_1 = C = 1, \hat{x}_2 = D = 1\). But we can also make a line as \(b = ct + d\), where \(\hat{x}_1 = C = 0, \hat{x}_2 = D = 2\) is also correct. We will have a different answer when we get to the pseudoinverse but here, our shortest solution will be \(x^+ = (1, 1)\). This is because \(x^+\) has the length of \(\sqrt{2} \), whereas the solution \(\hat{x} = (0, 2) \text{ or } (2, 0)\) have a length of 2. 

\subsection{Fitting by a Parabola} 

When we throw a ball, the most accurate approximation would be  a parabola \(b = C + Dt + Et^2\) so that it can come up and down. But even with non-linear function like \(t^2\), the unknowns \(C, D, E\) still appear linearly. 

\textbf{Problem} Try to fit heights \(b_1, \ldots, b_m\) at times \(t_1, \ldots, t_m\) by parabola \(C + Dt + Et^2\)

\textbf{Solution} 
\[
    \begin{bmatrix}
         C + Dt_1 + Dt^2_1 = b_1 \\
         \vdots \\
         C + Dt_m + Dt^2_m = b_m \\
    \end{bmatrix}
    \text{ is Ax = b with}
    \begin{bmatrix}
        1 & t_1 & t^2_1  \\
        \vdots & \vdots & \vdots  \\
        1 & t_m & t^2_m   \\
    \end{bmatrix}
\]

So what we are dealing with here is a 3 dimensional column space of \(A\). Where the projection of \(b\) is \(p = A\hat{x}\), with three columns of coefficient \(C, D, E\). The error at the data points are \(e_m = b_m - C - Dt_m - Et^2_m\). We can minimize it by calculus by taking partial derivative of \(e^2_1 + \ldots + e^2_m\) with respect to \(C, D, E\). 

\textbf{Example 3:} For the the parabola \(b = C + Dt + Et^2\) to go through the three heights \(b = 6, 0, 0\) when \(t = 0, 1, 2\) the equation for \(C, D, E\) are 
\[
    \begin{bmatrix}
         C + D \cdot 0 + E \cdot 0^2 = 6\\
         C + D \cdot 1 + E \cdot 1^2 = 0 \\
         C + D \cdot 2 + E \cdot 2^2 = 0 \\
    \end{bmatrix}
\]     

We can solve this directly for \(x = (C, D, E) = (6, -9, 3)\). The parabola through the three points is \(b\)  

Here are a few important points to remember:
\begin{enumerate}
    \item The least square solution \(\hat{x}\) minimizes the \(\lVert Ax - b \rVert^2 \) = \(x^T A^T Ax - 2x^T A^T b + b^T b\). That is E, the sum of squares of the errors \(e_1, \ldots, e_m\) in the \(m\) equations \(m > n\). 
    \item The best \(\hat{x}\) comes from the normal equations \(A^T A \hat{x} = A^T b\). E is a minimum. 
    \item To fit \(m\) points by a line \(b = C + Dt\), the normal equations give \(C and D\). 
    \item The heights of the best line are \(p = p_1, \ldots, p_m\). The vertical distance to the data points are the errors \(e = e_1, \ldots, e_m\). A key equation is \(A^T e = 0\). 
    \item If we try to fit \(m\) by a combination of \(n < m\), the \(m\) equations \(Ax = b\) are generally unsolvable. The equation \(A^T A\hat{x} = A^T b\) give the least square solution, the combination with the smallest mean square error (MSE).               
\end{enumerate}

\section{Orthogonal Matrices and Gram-Schmidt}

\begin{enumerate}
    \item The columns \(q_1, \ldots, q_n\) are orthogonal if \(q^T_i q_j = \begin{bmatrix}
         0 \text{ for  \(i \neq j \) } \\
         1 \text{ for  \(i = j \) } \\
    \end{bmatrix}\). Then \(Q^T Q = I\). 
    \item If \(Q\) is also square, then \(QQ^T = I\) and \(Q^T = Q^{-1} \). Now \(Q\) is an orthogonal matrix. 
    \item The least squares solution to \(Qx = b \text{ is } \hat{x} = Q^T b\). Projection of \(b: p = QQ^T b = Pb\).
    \item The \textbf{Gram-Schmidt} process takes independent \(a_i\) to orthonormal \(q_i\). Start with \(q_1 = \frac{a_i - \text{ its projection } p_i}{\lVert a_1 - p_i \rVert }\); projection \(p_i = (a^T_i q_1 + \ldots + a^T_i q_{i-1} )q_{i-1} \). 
    \item Each \(a_i\) will be a combination of \(q_1\) to \(q_i\). Then \(A = QR\) othrogonal \(Q\) and triangular \(R\).      
\end{enumerate}

We have two goal in this section, \textbf{why} and \textbf{how} . We will first see why orthogonal column in \(A\) are good. The second goal is to construct orthogonal vectors \(q_i\). The orthonormal basis vectors \(q_i\) will be the columns of a new matrix \(Q\). The vector \(q_1, \ldots, q_n\) are orthogonal when their dot products \(q_i \cdot q_j\) are zero, more exactly \(q^t_i q_j = 0\) whenever \(i \neq j\). With one more step, we can divide each vector by its length for the vectors to become \emph{orthogonal unit vector}. All their length are 1 (called \textbf{normal} ), then the basis is called \textbf{orthonormal}.          

\textbf{Definition} the \(n\) vectors \(q_1, \ldots, q_n\) are orthogonal if 

\[
    q^T_i q_j = 
    \begin{dcases}
      0, & \text{if } i \neq j \text{ (orthogonal vectors)}; \\
      1, & \text{if } i = j \text{ (unit vectors, } \lVert q_i \rVert = 1 \text{)}
    \end{dcases}
\]

A matrix \(Q\) with orthonormal columns has \(Q^T Q = I\), typically \(m > n\). 

Matrix \(Q\) is easy to work with because \(Q^T Q = I\), which means that the columns \(q_1, \ldots, q_n\) are orthonormal. Q is not required to be square. 

\[
    Q^T Q = 
    \begin{bmatrix}
         q^T_1 \\
         q^T_2 \\
         \vdots \\
         q^T_n \\
    \end{bmatrix}
    \begin{bmatrix}
        q^T_1 & q^T_2 & \ldots & q^T_n  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        1 & 0 & \ldots & 0  \\
        0 & 1 & \ldots & 0  \\
        \vdots & \vdots & \ddots & \vdots  \\
        0 & 0 & \ldots & 1  \\
    \end{bmatrix}
    = I 
\]

When row \(i\) of \(Q^T\) multiples column \(j\) of \(Q\), the dot product is \(q^T_i q_j\). Off the diagonal \((i \neq j)\), the dot is zero by orthogonality. On the diagonal \((i - j)\) the unit vector gives \(q^T_i q_i = \lVert q_i \rVert^2 = 1\). 
\[
    \text{When \(Q\) is square, \(Q^T Q = I\) means that \(Q^T = Q^-1\): tranpose = inverse.   }
\]       

If the column are only orthogonal and not unit vector, they still give non-1 diagonal matrix. The inverse is the transpose of \(Q^T\). In the case it's square, we call it orthogonal matrix.

\textbf{Example 1 (Rotation):} \(Q\) rotates every vector in the plane by the angle \(\theta \): 
\[
    Q = 
    \begin{bmatrix}
        \cos \theta   & -\sin \theta   \\
        \sin \theta  & \cos \theta   \\
    \end{bmatrix}
    \text{ and }
    Q^T = Q^{-1} 
    \begin{bmatrix}
        \cos \theta  & \sin \theta   \\
        - \sin \theta  & \cos \theta   \\
    \end{bmatrix}
\]   

The columns of \(Q\) are orthogonal. They are unit vectors because \(\sin \theta ^2 + \cos \theta ^2 = 1\), giving us an orthonormal basis for the plane \(R^2\). The standard basis vectors \(i\) and \(j\) are rotated through \(\theta \). Then \(Q^{-1} \) rotate back through \(-\theta \). It does agrees with \(Q^T\) because \(\cos -\theta = \cos  \theta \) and \(\sin  -\theta = -\sin \theta   \). We have \(QQ^T = I = Q^T Q\). 

\textbf{Example 2 (Permutation): } The matrices change the order to \((y, z, x)\) and \((y, x)\):
\[
    \begin{bmatrix}
        0 & 1 & 0  \\
        0 & 0 & 1  \\
        1 & 0 & 0  \\
    \end{bmatrix}
    \begin{bmatrix}
         x \\
         y \\
         z \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         y \\
         z \\
         x \\
    \end{bmatrix}
    \text{ and } 
    \begin{bmatrix}
        0 &  1 \\
        1 &  0 \\
    \end{bmatrix}
    \begin{bmatrix}
         x \\
         y \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         y \\
         x \\
    \end{bmatrix}
\]   

The column of these \(Q\) are orthogonal and also unit vectors. \emph{The inverse of a permutation matrix is its transponse: \(Q^{-1} = Q^T \) }. 
\[
    \begin{bmatrix}
        0 & 0 & 1  \\
        1 & 0 & 0  \\
        0 & 1 & 0  \\
    \end{bmatrix}
    \begin{bmatrix}
         y \\
         z \\
         x \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         x \\
         y \\
         z \\
    \end{bmatrix}
    \text{ and }
    \begin{bmatrix}
        0 & 1  \\
        1 &  0 \\
    \end{bmatrix}
    \begin{bmatrix}
         y \\
         x \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         x \\
         y \\
    \end{bmatrix}
\]  
\[
    \text{\textbf{Every permutation matrix is an orthogonal matrix.} }
\]

\textbf{Example 3 (Reflection):} If \(u\) is any unit vector, set \(Q = I - 2uu^T\). Notice that \(uu^T\) is a matrix while \(u^T u\) is the number \(\lVert u \rVert^2 = 1 \). Then \(Q^T and Q^{-1} \) both equal \(Q\): 
\[
    Q^T = I - 2uu^T = Q 
    \text{ and }
    Q^T Q = I - 4uu^T + 4uu^T uu^T = I 
\]        
Reflection matrix \(I - 2uu^T\) are symmetric and orthogonal. If we square them, we get the identity matrix: \(Q^2 = Q^T Q = I\), where reflecting the mirror twice bring us back to the original \((-1)^2 \rightarrow 1\) 
Check figure 4.9, page 178. we achieve rotation by \(Q = \begin{bmatrix}
    c & -s  \\
    s & c  \\
\end{bmatrix}\) and reflection by \(Q = \begin{bmatrix}
    1 & 0  \\
    0 & 1 \\
\end{bmatrix}\) 

Say, we choose the direction \(u = (\frac{-1}{\sqrt{2} },\frac{1}{\sqrt{2} })\), compute \(2uu^T\) and subtract it from \(I\) to get reflection \(Q\): 
\[
    Q = I - 2
    \begin{bmatrix}
        .5 & -.5  \\
        -.5 & .5  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        0 & 1  \\
        1 & 0  \\
    \end{bmatrix}
    \text{ and }
    \begin{bmatrix}
        0 & 1  \\
        1 & 0  \\
    \end{bmatrix}
    \begin{bmatrix}
         x \\
         y \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         y \\
         x \\
    \end{bmatrix}
\]    
So when we change \(x, y\) to \(y, x\), vectors like \(3, 3\) or on the diagonal doesn't move because it is on the mirror line. It tells us that rotation, reflection, and permutations doesn't change the length of any vectors. 

\textbf{Proof} \(\lVert Qx \rVert^2 = \lVert x \rVert^2  \) because \((Qx)^T (Qx) = x^T Q^T Qx = x^T Ix, x^T x\). 
\[
    \text{If \(Q\) has orthonormal columns, it leaves length unchanged. Same length for \(Qx\) too. \(\lVert Qx \rVert = \lVert x \rVert\) for every \(x\)   }
\]  

\(Q\) also preserves dot products, \((Qx)^T (Qy)=x^T Q^T Qy = x^T y\), or just use \(Q^T Q = I\).  

\subsection{Projection \(QQ^T\) Using Orthonormal Bases: \(Q\) Replaces A}


For projection onto subspaces all far, all formula involves \(A^T A\), where the entries are dot products \(a^T_i a_j\) of the basis vector \(a_1, \ldots, a_n\). 
Say, \emph{what if these basis vectors orthonormal?} The \(a\)'s become \(q\)'s. Then \(A^T A\) simplies to \(Q^T Q = I\). Look at the improvement for \(\hat{x}, p, P\). 
\[
    \hat{x} = Q^T b, 
    p = Q\hat{x}, 
    P = QQ^T 
\]        

\emph{The least squares solution of \(Qx = b\) is \(\hat{x} = Q^T b\)}. The projection matrix if \(QQ^T\). 

\[
    \text{Projection onto \(q\)'s }, 
    p = 
    \begin{bmatrix}
        q_1 & \ldots & q_n  \\
    \end{bmatrix}
    \begin{bmatrix}
         q^T_1 b \\
         \vdots \\
         q^T_n b \\
    \end{bmatrix}
    = q_1(q^T_1 b) + \ldots + q_n(q^T_n b)
\]

\textbf{Important}, when \(Q\) is square and \(m = n\), the subspace is whole space. Meaning that \(Q^T = Q^{-1} \) and \(\hat{x} = Q^T b\) is the same as \(x = Q^{-1} b \), the solution are exact. The projection of \(b\) onto the whole space is \(b\) itself. So \(p = b\) and \(P = QQ^T = I\). 

Now that doesn't sound like much, but when \(p = b\), then a formula assembles  \(b\) out of its 1-dimensional projections. If \(q_1, \ldots, q_n\) is an orthonormal basis for the whole space, then \(Q\) is square. Every \(b = QQ^T b\) is the sum of its component along the \(q\)'s:

\[
    b = q_1(q^T_1 b) + \ldots + q_n(q^t_n b)
\]

\textbf{Transform} \(QQ^T = I\) is the foundation of Fourier series and all the great transform. It breaks \(f(x)\) into perpendicular pieces then adding the pieces like the equation above, inverse transform puts \(b\) and \(f(x)\) back together. 

\textbf{Example 4:} The columns of this orthogonal \(Q\) are orthonormal vectors \(q_1, q_2, q_3\):
\[
    m = n = 3, Q = \frac{1}{3}
    \begin{bmatrix}
        -1 & 2 & 2  \\
        2 & -1 & 2  \\
        2 & 2 & -1  \\
    \end{bmatrix}
    \text{ has }
    Q^T Q = Q Q^T = I 
\]   

Separate projection \(b\) onto \(q_1, q_2, q_3\) are \(p_1, p_2, p_3\):
\[
    q_1(q^T_1 b) = \frac{2}{3}q_1 
    , q_2(q^T_2 b) = \frac{2}{3}q_2
    , q_3(q^T_3 b) = -\frac{1}{3}q_3
\]   

The first two sum of projection \(p_1 + p_2\) projects \(b\) onto the \textbf{plane} of \(q_1\) and \(q_2\). The sum of all three is the projection of \(b\) onto the whole space, where \(p_1 + p_2 + p_3 = b\):
\[
    \frac{2}{3}q_1 + \frac{2}{3}q_2 + \frac{1}{3}q_3
    = \frac{1}{9}
    \begin{bmatrix}
        -2 & +4 & -2  \\
        4 & -2 & -2  \\
        4 & +4 & 1  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         0 \\
         0 \\
         1 \\
    \end{bmatrix}
    = b 
\]      

\subsection{The Gram-Schmidt Process}

To make orthonormal vectors, we will need to start with independent vectors. Say \(a, b , c\). Will we turn these into orthogonal vectors \(A, B, C\). Then we will turn them orthonormal by \(q_1 = \frac{A}{\lVert A \rVert }, q_2 = \frac{B}{\lVert B \rVert }, q_3 = \frac{C}{\lVert C \rVert }\), and so on for other cases. 

We start by choosing \(A = a\), as the first direction. Next, \(B\) must be perpendicular to \(A\). We can do that by making \(b\) minus its projection on \(A\), which is just its \(e\), which is of course perpendicular. 

\[
    B = b - \frac{A^T b}{A^T A}A
\]

We should confirm that they are orthogonal by \(A^T B = 0\). If \(B\) is 0, that means \(A\) and \(B\) is dependent. The direction for \(A\) and \(B\) is now set and we can continue to \(C\). 
\[
    C = c - \frac{A^T c}{A^T A}A - \frac{B^T c}{B^T B}B
\]     

The core idea of Gram-Schmidt process is that we \emph{subtract from every new vector its projections in the direction already set.} If we had \(D\), we would minus \(d\)  with its projection on \(A, B, C\). Then to turn them orthonormal, we divide them by their norm length.

\textbf{Example}, suppose we have three independent non-orthogonal vectors \(a, b, c\):
\[
    a = 
    \begin{bmatrix}
         1 \\
         -1 \\
         0 \\
    \end{bmatrix}
    \text{ and }
    b = 
    \begin{bmatrix}
         2 \\
         0 \\
         -2 \\
    \end{bmatrix}
    \text{ and }
    \begin{bmatrix}
         3 \\
         -3 \\
         3 \\
    \end{bmatrix}
\] 

The math says, \(A = a, A^T A = 2, A^T b = 2\). Subtract \(b\) from its projection on \(A\). 
\[
    B = b - \frac{A^T b}{A^T A} = b - \frac{2}{2}A = 
    \begin{bmatrix}
         1 \\
         1 \\
         -2 \\
    \end{bmatrix}
\]   

Check whether \(A^T B = 0\) or not, it should. Now, we do the same to \(c\) on \(A, B\) for \(C\). 
\[
    C = c - \frac{A^T C}{A^T A}A - \frac{B^T C}{B^T B} = c - \frac{6}{2}A + \frac{6}{6}B = 
    \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
\]    

Check that \(C\) is perpendicular to \(A\) and \(B\). Then we convert them to unit, \(A, B, C\)'s length should be \(\sqrt{2}, \sqrt{6}, \sqrt{3}   \) respectively. 
\[
    q_1 = \frac{1}{\sqrt{2}}
    \begin{bmatrix}
         1 \\
         -1 \\
         0 \\
    \end{bmatrix}
    \text{ and }
    q_2 = \frac{1}{\sqrt{6} }
    \begin{bmatrix}
         1 \\
         1 \\
         -2 \\
    \end{bmatrix}
    \text{ and }
    q_2 = \frac{1}{\sqrt{3} }
    \begin{bmatrix}
         1 \\
         1 \\
         1 \\
    \end{bmatrix}
\]     

\(A, B, C\) usually contains fraction, \(q_1, q_2, q_3\) usually contains square roots. That is just the price we have to pay for this tool. 

\subsection{The Factorization \(A = QR\) }


Since we moved from \(A\) to \(Q\), and found that they are related, then what is the third matrix for their relation? It's \(R\) in \(A = QR\) , not the same \(R\) from chapter 1. If look back, we will see how the later \(q\)'s are not involved in the prior \(q\)'s
\begin{enumerate}
    \item The vector \(a, A, q_1\) are all along a single line. 
    \item The vector \(a, b, A, B, q_1, q_2\) are all in the same place. 
    \item The vector \(a, b, c, A, B, C, q_1, q_2, q_3\) are in one 3D subspace.  
\end{enumerate}       

We can see that the \(R\) is a triangular matrix. 
\[
    \begin{bmatrix}
        a & b & c  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        q_1 & q_2 & q_3  \\
    \end{bmatrix}
    \begin{bmatrix}
        q^T_1 a & qT^T_1 b & q^T_1 c  \\
         & q^T_2 b & q^T_2 c  \\
         &  & q^T_3 c  \\
    \end{bmatrix}
    \text{ or }
    A = QR
\]

Remember that \(R = Q^T A\) is upper triangular because later \(q\)'s are orthogonal to earlier \(a\)'s. 

\[
A = \begin{bmatrix}
1 & 2 & 3 \\
-1 & 0 & -3 \\
0 & -2 & 3
\end{bmatrix}
=
\begin{bmatrix}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}} & \frac{1}{\sqrt{3}} \\
0 & -\frac{2}{\sqrt{6}} & \frac{1}{\sqrt{3}}
\end{bmatrix}
\begin{bmatrix}
\sqrt{2} & \sqrt{2} & \sqrt{18} \\
0 & \sqrt{6} & -\sqrt{6} \\
0 & 0 & \sqrt{3}
\end{bmatrix}
= QR.
\]

We we look closely at the diagonal of \(R\), we can see the length of \(A, B, C = \sqrt{2}, \sqrt{6}, \sqrt{3}   \) there. 

We must remember that this is useful for least square. 
\[
    A^T A = (QR)^T QR = R^T Q^T QR = R^T R, A^T A\hat{x} = A^T b \text{ simpflies to } R^T R\hat{x} = R^T Q^T b, \text{ and finally } R\hat{x} = Q^T b 
\]

\[
    \text{Least squares: }
    R^T R\hat{x} = R^T Q^T b \text{ or } R\hat{x} = Q^T b \text{ or } \hat{x} = R^{-1} Q^T b 
\]

While \(Ax = b\) is impossible, \(R\hat{x} = Q^T b\) is very fast. Page 183 will have a code and logic worth looking through. 

\textbf{Key ideas:}
\begin{enumerate}
    \item If the orthonormal vectors \(q_1, \ldots, q_n\) are the columns of \(Q\), then \(q^T_i q_j = 0\) and \(q^T_i q_i = 1\), which translate to \(Q^T Q = I\). 
    \item If \(Q\) is square then \(Q^T = Q^{-1} \), transpose = inverse. 
    \item Length of \(Qx\) equals the length of x, \(\lVert Qx \rVert = x \)
    \item The projection onto the column space of \(Q\) spanned by the \(q\)'s is \(P\) = \(Q Q^T\). 
    \item If \(Q\) is square then \(P = Q Q^T = I\) and every \(b = q_1(q^T_1 b) + \ldots + q_n(q^T_n b)\). 
    \item Gram-Schmidt produces orthormal vectors \(q_1, q_2, q_3\) from independent \(a, b, c\). In matrix form, it is the factorization \(A = QR\)              
\end{enumerate}

\section{The Pseudoinverse of a Matrix}

\[
    r = m = n = 2 
    I = 
    \begin{bmatrix}
        1 & 0  \\
        0 &  1 \\
    \end{bmatrix}
    I = I^{-1}, (I)(I) = I 
\]

\[
    r = m < n = 3
    I_L = 
    \begin{bmatrix}
        1 & 0 & 0  \\
        0 & 1 & 0  \\
    \end{bmatrix}
    I_L = \text{left inverse} of I_R
    I_R = \text{right inverse} of I_L
\]

\[
    r = n < m = 3
    I_R = 
    \begin{bmatrix}
        1 & 0 \\
        0 & 1  \\
        0 & 0  \\
    \end{bmatrix}
    (I_L)(I_R) = I 
    (I_R)(I_L) \neq I
\] 

Only the first \(I\) is truly invertible. \(I_L\) and \(I_R\) have one-sided inverses but not true inverses. Every matrix \(A\) have a \textbf{pseudoinverse} \(A^+\). Which includes left inverse and right inverse. 

\emph{A has a left inverse:}
\begin{enumerate}
    \item \(A^+ A = I \) 
    \item \(A\) has independent columns 
    \item A can be tall and thin, \(r = n\)
    \item \(Ax = b\) can have no solution or one solution. 
    \item \(N(A)\) = zero vector 
    \item \(A^T A\) is \(n \times n\) and invertible. 
    \item The left inverse is \(A^+ (A^T A)^{-1} A^T \)      
\end{enumerate}

\emph{A has a right inverse:}
\begin{enumerate}
    \item \(A A^+ = I\)
    \item A has independent rows 
    \item A can be short and wide, \(r = m\)
    \item \(Ax = b\) can have one or many solutions 
    \item Left nullspace \(N(A)^T\) = zero vector 
    \item \(A A^T\) is \(m \times m\) and invertible 
    \item \(A^+ = A^T (A A^T)^{-1} \)      
\end{enumerate}

\(A^+ A = I\) describes the matrices in the least square chapter. The rank of \(r = n < m\) where \(Ax = b\) might be unsolvable. Then \(\hat{x} = A^+ b\) is the vector that solves \(A^T A\hat{x} = A^T b\) and makes the error \(e = b - A\hat{x}\) as small as possible, that is least square. 

\(A A^+ = I\) describes the opposite problem, \(r = m < n\), which means \(Ax = b\) has infinitely many solutions. In this case \(x^+ = A^+ b\) is the minimum length solution to \(Ax = b\) . The solution is in the row space of \(A\) and all other solutions \(x = x^+ + x_n\) have a nullspace component \(x_n\), which increases the length. 

\[
    \text{\(x^+\) is the minimum norm least squares solution to \(Ax = b\)  }
\]
\(x^+ = A^+ b\) also minimizes \(\lVert x \rVert^2 \). \emph{\(x^+\) has nullspace component = 0 }. If \(A\) has dependent columns, we need \(A^+\).

\subsection{The Pseudoinverse \(A^+\) \((n \times m)\)  of a Matrix \(A\) \((m \times n)\)}

To get this, we must remember a few things. 
\begin{enumerate}
    \item Every \(y\) in \(m\) dimensions has two perpendicular parts, \(y = b + z\).
    \item \(b\) is in the column space of \(A\) where \(z\) is in the nullspace of \(A^T\) .
    \item We can invert \(Ax = b\) to find \(A^+ b = x\).         
\end{enumerate}

\[
    \text{Pseudoinverse of \(A\) has \(A^+ b = x, A^+ z = 0, A^+ y = A^+ b + A^+ z = x\)  }
\]

If I were to put it into simple words, pseudoinverse is a practical, flawed, and yet the best undo button we have. It is guaranteed to exist, unlike a perfect inverse inverse. If you look on page 191 of Strang's, you will see that \(A^+\) looks at \(y = b + z\), and ignores \(z\) then send it off to an appropriate \(x\). However, in a case where \(y = b + 0\), it is the same since no matter what the \(z\) is, it is ignored. 

If we look at it, it is basically projection onto the row or col space. 
Therefore, each projection onto the row and col space have an equation. 
\[
    P_{row} = A^+ A = (A^+ A)^2 = (A^+ A)^T, 
    P_{col} = AA^+ = (A A^+)^2 = (A A^+)^T  
\]

\textbf{Example 1:} 
\[
    A = 
    \begin{bmatrix}
        2 &  0 \\
        0 &  0 \\
    \end{bmatrix}, 
    y = 
    \begin{bmatrix}
         y_1 \\
         0 \\
    \end{bmatrix}
    + 
    \begin{bmatrix}
         0 \\
         y_2 \\
    \end{bmatrix}
    = b + z , 
    A^+ = 
    \begin{bmatrix}
        \frac{1}{2} & 0  \\
        0 & 0  \\
    \end{bmatrix}
\] 

\(b = 
\begin{bmatrix}
     y_1 \\
     0 \\
\end{bmatrix}\) is in the column space of \(A\), \(Ax = b\) is \(\begin{bmatrix}
    2 & 0  \\
    0 & 0  \\
\end{bmatrix}
 \begin{bmatrix}
     \frac{y_1}{2} \\
     0 \\
 \end{bmatrix}
 = 
 \begin{bmatrix}
     y_1 \\
     0 \\
 \end{bmatrix}\)  

 \(z = \begin{bmatrix}
     0 \\
     y_2 \\
 \end{bmatrix}\) in the nullspace of \(A^T\), \(A^T z = 0\), you get the jazz. 
 
 Then it ends at \(A^+ y = A^+ b + A^+ z = x + 0 = \begin{bmatrix}
     \frac{y_1}{2} \\
     0 \\
 \end{bmatrix}\). Then \(A^+ = \begin{bmatrix}
    \frac{1}{2} & 0  \\
    0 & 0  \\
 \end{bmatrix}\)  
 where 
 \(A^+ A = \begin{bmatrix}
    1 &  0 \\
    0 &  0 \\
 \end{bmatrix}\). 

 Another useful usecase is diagonal matrix \(D\), square or rectangular: 
 \[
    \text{Pseudoinverse of } D = 
    \begin{bmatrix}
        2 & 0 & 0  \\
        0 & 3 & 0  \\
        0 & 0 & 0  \\
    \end{bmatrix}
    \rightarrow
    D^+ = 
    \begin{bmatrix}
        \frac{1}{2} & 0 & 0  \\
        0 & \frac{1}{3} & 0  \\
        0 & 0 & 0  \\
    \end{bmatrix}
 \] 

 Remember though, \(\frac{1}{0}\) must be zero. This also extends from a diagonal \(D\) to any matrix \(A = BDC\). If \(B\) and \(C\) are invertible then \(A^+ = B^{-1} D^+ C^{-1}\), and for completion, if only \(B\) is invertible, then \(A^+ = B^+ D^{-1 C^{-1} } \)       

 Is there a general solution Yes, SVD. But that is still far off. For now, we are only concerned with solution for specific cases. Tall thin matrices, and short wide matrices. Most matrices do not have a \(A^{-1} \) where \(A^{-1} A = I \) and \(A A^{-1} = I \), only square invertible matrices can do that.

 For tall thin matrices where the columns are independent, \(A^+ = (A^T A)^{-1} A^T \) is the left inverse of \(A\). Which makes \(A^+ A = I\), but \(A A^+ \neq I\). 
 
 Then for short wide matrices where the rows are independent, \(A^+ = A^T (A A^T)^{-1} \), as the right inverse of \(A\). Here, \(A A^+ = I\), but \(A^+ A \neq I\).  

