\chapter{The Four Fundamental Subspaces}

How can we define "vector space"? Well, if we are talking about \(R^3\), the key operation are \(v + w \text{ and } cv\).
Notice that v and w could be matrices, so we could have matrix spaces and function spaces. Then inside \(R^n\) we could only allow \(x\) that satisfies \(Ax = 0\), which will produce "nullspace of A".
All combination of solution to \(Ax = 0\) are also solutions, meaning that the nullspace is a subspace. To point it simply, nullspace is just \(x \text{ in } Ax \text{ that crushes it down to } 0\). Why is it called a space? Because it has structures and rules. 
\begin{enumerate}
    \item It must contain zero vector, \(A \times 0 = 0\) always.
    \item It must be closed under addition, meaning that if you take two vectors from the nullspace then add them together, they must still be in the nullspace.
    \item It must be closed under scalar addition, meaning that if you take any vector from the nullspace then multiplies them by a constant, the result must stil be in the nullspace. 
\end{enumerate}
Then, lastly, there are basis. A set of vectors that perfectly describes the space. Very important, some considers it the fundamental theorem. So, what it means is that basis are just sets of movement vectors for each dimension.
\begin{enumerate}
    \item For a 3D space, you will need forward, right, and up.
    \item For a 2D space, you will need right and up.
    \item For a 1D space (line of sight, nullspace), you only need the direction of the line.
\end{enumerate}

And so, \(n - r\) special solutions to \(Ax = 0\) are a basis for \(N(A)\). It's called rank-nullity theorem.
\[
    \text{Dimension of Input Space = Dimension of Column Space + Dimension of Nullspace, }
    n = r + (n - r)
\] 
Note that \(n - r\) is the dimension of the nullspace. 
One last note: THIS CHAPTER IS VERY IMPORTANT.
\section{Vector Spaces and Subspaces}

Here are a few fundamental points:
\begin{enumerate}
    \item All linear combination of \(cv + dw\) must stay in the vector space (What is a vector space? Very simply, all the possible spaces tha can be achieved by your given vector with vector addition or scalar multiplication), where \(c\) and \(d\) are scalar, and \(v \text{ and } w\) are vectors.
    \item The row space (meaning all possible linear combination of the row vectors) is "spanned" (made up of) rows of \(A\). While the column of \(A\) spans \(C(A)\).
    \item Matrices can be filled by more than just numbers. As long as it obeys the rules of a vector space, it can be treated as a vector. 
    For example, we have two equations, \(f(x) = x^2\) and \(g(x) = 2x\). Can they be added together? 
    Yes! \(h(x) = x^2 + 2\). Now if we fill it with the likes of \(\sin, \cos, x, x^2\), we can "span" and build a lot of other functions. 
    For example, all quadratic polynomials are "spanned" by the functions \(f_1(x) = 1\), \(f_2(x) = x\), and \(f_3(x) = x^2\)       
\end{enumerate}

\(R^n\) contains all column vector \(v\) to the length of \(n\). For this case, the components from \(v_1 \text{ to } v_n\) are all real numbers. However, if they allow for complex numbers \((i)\), the \(R^n \text{ becomes } C^n\). 
To reiterate, all linear combination of \(cv + dw\) must be in the vector space \(R^n\).
For example, all positive the set with all positive (meaning no vector consist of ANY nonpositive numbers) vectors \((v_1, \ldots, v_n)\) areNOT a vector space. 
Why? Simply take one simple vector, say \((1, 2)\) 
then multiply it by scalar of, say, c = -1. 
\((-1, -2)\) is NOT in our set, therefore it is not a vector space.
Or for another example, a set of solution for \(Ax = (1, \ldots, 1)\) is not a vector space because a line in \(R^n\) is not a vector space unless it goes the central point \((0, \ldots, 0)\).

\subsection{Examples of Vector Spaces}

Here are some examples of a neat vector space, the \(Z\) (zero vector) where \(0 = (0, 0, \ldots, 0)\). Combinations of \(c0 + d0\) are all still 0, so  still in the subspace.
How about vector space of matrices? We can do that. \(R^{3 \times 3} \) is a space that contains all \(3 \times 3\) matrices. It does satisfy all eight rules, so why not? It's also a vector space. 
How about a vector space of functions? Sure can. The line of functions \(y = ce^x\) (any c) is a line in a function space. 
This line contains all solutions to the differential equations of \(\frac{dy}{dx} = y\). Yet another function space contains all quadratics \(y = a + bx + cx^2\), where they are the solutions to \(\frac{d^3 y}{dx^3}\)
And to reiterate, space in this context means all possible linear combination of the vectors or matrices or functions, and they all stay inside it.

\subsection{Subspaces of Vector Spaces}

What ar subspaces? To put it simply, they are a flat plane inside the dimensional space, however, they are still the same dimension. Let's say, we got a \(R^3\) space. We can make a plane any way we want as long as it passes \((0, 0, 0)\), what we get may look like a 2D plane, but it's still 3D. Therefore, the plane is a subspace of  the full vector space \(R^3\).
\newline
Here is a list of possible subspaces of \(R^3\):
\begin{enumerate}
    \item Any line through \((0, 0, 0)\)
    \item Any plane through \((0, 0, 0)\)
    \item The whole space \(R^3\) 
    \item The zero vector \((0, 0, 0)\) 
\end{enumerate}

\subsection{The Column Space of A}

What we are trying to solve here is \(Ax = b\). We want to know b, right? Well, \(b\)  are a column space of \(A\). \(Ax\) is just a combination of A, and to get every possible \(b\), we need all possible \(x\), which is just all linear combination of \(A\), which is the column space of \(A\), as written earlier. To build on that, vector space is made up of column vectors.       

A crucial point to understand is that, \textbf{to solve \(Ax = b\) is just to express \(b\) as a combination of the columns}. \(b\) got to be in the column space of A, otherwise, it doesn't exist!

Caution: columns of A do not form a subspace. Neither do invertible matrices, or singular matrices. Only all linear combinations.

\subsection{The Row Space of A}

\textbf{The rows of A are the column of \(A^T\)}, why do we do this?
Because we like working with columns, so we use the column of \(A^T\)

\[
    \text{\textbf{The row space of A is just the column space of \(A^T\) }}
\]  



\section{Computing the Nullspace by Elimination: \(A = CR\) }

\begin{enumerate}
    \item The \textbf{nullspace N(A)} in \(\mathbf{R^n}\) contains all solutions x to \(Ax = 0\), including \(x = 0\).
    \item CONTINUE THIS LATER      
\end{enumerate}

The goal of this section is to find all solutions for \(Ax = 0\). 
If \(A\) is an invertible matrix, then the only solution is \(x = 0\). In general, A has \(r\)  independent columns, the other \(n - r\) are a linear combination. 
\newline
Here is a matrix \(R\) with rank \(r = 2\), with \(n = 4\) columns. This means we have \(n - r = 4 - 2 = 2\) independent solutions to \(Rx = 0\). So the nullspace \(N(R)\) will have 2 dimensions.
\[
    \text{\textbf{Example 1: }}
    R = [I F] P = \begin{bmatrix}
        1 & 0 & 3 & 5  \\
        0 & 1 & 4 & 6  \\
    \end{bmatrix}
\]

Which means \(Rx = 0\) is \(x_1 + 3x_3 + 5x_4 = 0\) and \(x_2 + 4x_3 + 6x_4 = 0\).
We find the special solutions by just letting \(x_3 \text{ and } x_4\) equal 1 and 0 or 0 and 1.
Set \(x_3 = 1, x_4 = 0\) the equations will give \(x_1 = -3, x_2 = -4\)    
Set \(x_3 = 0, x_4 = 1\) the equations will give \(x_1 = -5, x_2 = -6\)
This gives us two special solutions: \(s_1 = (-3, -4, 1, 0) \text{ and } s_2 = (-5, -6, 0, 1)\). They are both in the nullspace of \(R\), as we can also see, \(cs_1 + ds_2\) is still in the the nullspace. And so, \(s_1 \text{ and } s_2\) are the basis of nullspace.

\[
    \text{\textbf{Example 2: }} 
    R_0 =
    \begin{bmatrix}
        1 & 7 & 0 & 8  \\
        0 & 0 & 1 & 9  \\
        0 & 0 & 0 & 0  \\
    \end{bmatrix}
\]

Which means \(x_1 + 7x_2 + 0x_3 + 8x_4 = 0\), \(x_3 + 9x_4 = 0\), and \(0 = 0\) (wow on the last one). The matrix identity is inside column 1 and 3, and row 3 is all zero, which makes it a reduced row echelon form, even elimination can't make it simplier. We still have free variables for the special solution, namely, \(x_2 \text{ and } x_4\).   
Set \(x_2 = 1, x_4 = 0\), the equations give \(x_1 = -7, x_3 = 0\).
Set \(x_2 = 0, x_4 = 1\), the equations give \(x_1 = -8, x_3 = -9\).
The special solutions are noe \(s_1 = (-7, 1, 0, 0)\) and \(s_2 = (-8 ,0 , -9, 1)\).

First, we start with any \(m \text{ by } n\) matrix A, then apply elimination. That changes A into its reduced row echelon form, \(R_0 = \text{rref} (A)\). Removing all zero rows of \(R_0 \text{ leaves } R\).
\[
    r, m, n = 2, 2, 4 \text{ Simplest Case } R = [I F]
    \text{ as in}
    \begin{bmatrix}
        1 & 0 & 3 & 5  \\
        0 & 1 & 4 & 6  \\
    \end{bmatrix}
\]    

\[
    r, m, n = 2, 3, 4 \text{ General Case } 
    R_0 =
    \begin{bmatrix}
        I & F  \\
        0 & 0  \\
    \end{bmatrix}
    P
    \text{ as in }
    \begin{bmatrix}
        1 & 7 & 0 & 8  \\
        0 & 0 & 1 & 9  \\
        0 & 0 & 0 & 0  \\
    \end{bmatrix}
\] 
Hold up, what is rref, I, F here?
For something to be rref, you need to satisfy 4 conditions.
\begin{enumerate}
    \item The zero row is at the botto
    \item The first non-zero in row 1 is 1, the first non-zero entry in row 2 is 1. 
    \item Each pivot is to the right of the pivots in the rows above it.
    \item Every pivot is only non-zero number in its entire column.
\end{enumerate}

\(I\) in this context is still identity, but only if you take the pivot columns and align them chronologically as they were.
\(F\) is free matrix, the other non-pivot columns, still in the same chronologically order.

In this case \((R_0)\),
\[
    \text{I is }
    \begin{bmatrix}
        1 & 0  \\
        0 & 1  \\
        0 & 0  \\
    \end{bmatrix}
    , 
    \text{ F is }
    \begin{bmatrix}
        7 & 8  \\
        0 & 9  \\
        0 & 0  \\
    \end{bmatrix}
\]

\subsection{Elimination from \(A\)  to rref \((A)\): Reduced Row Echelon Form}

Refresher, how does elimination works?
\begin{enumerate}
    \item Subtruct a multiple of one row from another row
    \item Multiple a row by nonzero number
    \item Exchange any rows
\end{enumerate}
For demonstration,
\[
    A =
    \begin{bmatrix}
        1 & 2 & 11 & 17  \\
        3 & 7 & 37 & 57  \\
    \end{bmatrix}
    \text{ then }
    \begin{bmatrix}
        1 & 2 & 11 & 17  \\
        0 & 1 & 4 & 6  \\
    \end{bmatrix}
    \text{ then }
    \begin{bmatrix}
        1 & 0 & 3 & 5  \\
        0 & 1 & 4 & 6  \\
    \end{bmatrix}
\]
So, what did elimination actually do? It inverted the leading 2 by 2 matrix, which we will call W.
\[
    W =
    \begin{bmatrix}
        1 & 2  \\
        3 & 7  \\
    \end{bmatrix}
    \text{ into }
    \begin{bmatrix}
        1 & 0  \\
        0 & 1  \\
    \end{bmatrix}
\]
We multiplied \(W^{-1}A = W^{-1}[ W H ] \text{ to produce } R =  [I W^{-1} H ] = [ I F ]  \). We always knew that free columns \((H)\) is some combination of independent columns \((W)\), but we now know that \(H = W F\).
\[
    H =
    \begin{bmatrix}
        11 & 17 \\
        37 & 57  \\
    \end{bmatrix}
    = W F =
    \begin{bmatrix}
        1 & 2  \\
        3 & 7  \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
        3 & 5  \\
        4 & 6  \\
    \end{bmatrix}
\]
\textbf{However you compute R from A, you will always get the same R. R is completely controlled by A.}

For \textbf{example 2}, let us rref another \(A\). 
\[
    A = 
    \begin{bmatrix}
        1 & 7 & 3 & 35  \\
        2 & 14 & 6 & 70  \\
        2 & 14 & 9 & 97  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 7 & 3 & 35  \\
        0 & 0 & 0 & 0  \\
        0 & 0 & 3 & 27  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 7 & 0 & 8  \\
        0 & 0 & 0 & 0  \\
        0 & 0 & 3 & 27  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 7 & 0 & 8  \\
        0 & 0 & 1 & 9  \\
        0 & 0 & 0 & 0  \\
    \end{bmatrix}
    = R_0
\] 

\subsection{Elimination Column by Column: The Steps from A to \(R_0\) }

We will now reduce what we learn to an easily applied algorithm. 
The big question is \textbf{does this new column \(k + 1\) join with \(I_k \text{ or } F_k\)?}

\textbf{If \(l\) is all zero }, the new column is \textbf{dependent} on the first \(k\) columns. Then \(\mathbf{u} \) joins with \(F_k\) to become \(F_{k+1} \). 

\textbf{If \(l\) is not all zero }, then it is independent of the first k columns. Use the \textbf{largest} number, preferably, in \(l\) as the pivot. \textbf{Important} thing to remember here is that the column are talking about means the columns \textbf{UNDERNEATH} all pivots, not \textbf{ALL} columns. Then do elimination. Whatever is left becomes part of \(\mathbf{I} \) as \(\mathbf{I_{k+1} }  \). 
 
From example 2, we can see that the combination of independent and dependent comes out to 
\[
    C \times F = 
    \begin{bmatrix}
        1 & 3  \\
        2 & 6  \\
        2 & 9  \\
    \end{bmatrix}
    \begin{bmatrix}
        7 & 8  \\
        0 & 9  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        7 & 35  \\
        14 & 70  \\
        14 & 97  \\
    \end{bmatrix}
    =
    \text{depend columns of 2 and 4 of }A
\]

Right back to where we came from, showing that \(C\) are almost like the ingredients and \(F\) are almost like the method. 

\subsection{The Matrix Factorization \(A = CR\) and the Nullspace}

In chapters prior, we know that \(A = CR\) but we have no systemic way to find them, now we do. We apply elimination to reduce \(A \text{ to } R_0\). Then \(I\) in \(R_0\) locates the matrix C of independent columns in \(A\). Removing zero row in \(R_0\) produces \(R\) for \(A = CR\)

We have two special solution \(s_1\) and \(s_2\) for every column of \(F\) in \(R\).

\[
    Rs_1 = 0,
    \begin{bmatrix}
        1 & 7 & 0 & 8  \\
        0 & 0 & 1 & 9  \\
    \end{bmatrix}
    \begin{bmatrix}
        -7 \\
        1  \\
        0  \\
        0  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        0 \\
        0 \\
    \end{bmatrix}
\]
\[
    Rs_1 = 0,
    \begin{bmatrix}
        1 & 7 & 0 & 8  \\
        0 & 0 & 1 & 9  \\
    \end{bmatrix}
    \begin{bmatrix}
        -8 \\
        0  \\
        -9  \\
        0  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        0 \\
        0 \\
    \end{bmatrix}
\]
\(s_1\) and \(s_2\) are the easiest to see using the matrices \(-F\) and \(I\) and \(P^T\). 

\[
    \text{As a reminder, the two special solutions to }
    [I F] P x = 0
    \text{ are the columns of }
    P^T 
    \begin{bmatrix}
         -F \\
          I \\
    \end{bmatrix}
\]
It more correct than the other option because \(P P^T\) is the identity matrix of permutation matrix P:
\[
    \mathbf{Rx = 0, }
    [I F] P 
    \times
    P^T 
    \begin{bmatrix}
         -F \\
         I \\
    \end{bmatrix}
    \text{ reduces to }
    [I F]
    \begin{bmatrix}
         -F \\
          I \\
    \end{bmatrix}
    = 
    [0]
\] 

\textbf{Review} Say, the \(m\) by \(n\) matrix \(A\) has rank \(r\). We can find \(n - r\) special solution to \(Ax = 0\)  by comuting the rref \(R_0\) of \(A\). Remove the \(m - r\) zero rows of \(R_0\) to produce \(R = [I F] P\) and \(A = CR\). Then the special solutions to \(Ax = 0\) are the \(n - r\) columns of \(P^T [-F, I]^T\)  

\textbf{Example 3: Elimination of \(A\) gives \(R_0\) and \(R\). \(R\) reveals the nullspace of \(A\) } 
\[
    A =
    \begin{bmatrix}
        1 & 2 & 1  \\
        2 & 4 & 5  \\
        3 & 6 & 9  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 2 & 1  \\
        0 & 0 & 3  \\
        0 & 0 & 6  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 2 & 0  \\
        0 & 0 & 1  \\
        0 & 0 & 0  \\
    \end{bmatrix}
    = R_0
    \text{ with rank 2}
\]
\[
    R = 
    \begin{bmatrix}
        1 & 2 & 0  \\
        0 & 0 & 1  \\
    \end{bmatrix}
\]
The independent columns are 1 and 3. 

To solve \(Ax = 0\) and \(Rx = 0\), set \(x_2 = 1\), which will get you \(x_1 = -2, x_3 = 0\). Leave us special solution:
\[
    \mathbf{s = (-2, 1, 0)}  
\]  
All solutions \(x = (-2c, c, 0)\). And here it is, \(A = CR\)
\[
     A =
    \begin{bmatrix}
        1 & 2 & 1  \\
        2 & 4 & 5  \\
        3 & 6 & 9  \\
    \end{bmatrix}
    = CR =
    \begin{bmatrix}
        1 & 1  \\
        2 & 5  \\
        3 & 9  \\
    \end{bmatrix}
    \begin{bmatrix}
        1 & 2 & 0  \\
        0 & 0 & 1  \\
    \end{bmatrix}
    = 
    \text{columns basis in \(C\) } \times \text{row basis in \(R\).}
\]  

For a lot matrices, the only solution to \(Ax = 0\) is \(x = 0\). Simply, all columns of \(A\) are independent. The nullspace \(N(A)\) contains only the zero vector, no special solution. This case zero nullspace is \textbf{important} because it means that all columns of \(A\) is independent. But this can't happen if \(n > m\) (column > row) because you can have \(n\) independent column in \(R^m\).   

\textbf{Important} Say \(A\) has more columns than rows \((n > m)\), there will be at least one free variable. Meaning that \(Ax = 0\) has at least one non-zero solution. Or to put it more specifically, there must be more than \(n - m\) free columns. \(Ax = 0\) must have nonzero solutions in \(N(A)\). 

\medbreak

\textbf{Example 4: Find the nullspace of A, B, M and the two special solutions to \(Mx = 0\) }
\[
    A = 
    \begin{bmatrix}
        1 & 2  \\
        3 & 8  \\
    \end{bmatrix}
    ,  
    B =
    \begin{bmatrix}
         A \\
         2A \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        1 & 2  \\
        3 & 8  \\
        2 & 4  \\
        6 & 16  \\
    \end{bmatrix}
    ,
    M = 
    \begin{bmatrix}
        A & 2A  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        1 & 2 & 2 & 4  \\
        3 & 8 & 6 & 16  \\
    \end{bmatrix}
\]
\textbf{Solution} The equation \(Ax = 0\) has only the zero solution \(x = 0\). The nullspace is only \(Z\). 
\[
    Ax = 
    \begin{bmatrix}
        1 & 2  \\
        3 & 8  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 2  \\
        0 & 2  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 0  \\
        0 & 1  \\
    \end{bmatrix}
    = R = I
\]   

No free variables, meaning A is invertible; therefore, no special solution. 

The M matrix is different. It has extra columns instead of rows. That means that, with 4 columns and 2 rows, there will be 2 free columns leftover. 
\[
    M = 
    \begin{bmatrix}
        1 & 0 & 2 & 0  \\
        3 & 8 & 6 & 16  \\
    \end{bmatrix}, 
    R = 
    \begin{bmatrix}
        1 & 0 & 2 & 0  \\
        0 & 1 & 0 & 2  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        I & F  \\
    \end{bmatrix}
\]
Again, to get special solutions out of it, we will let \(x_3 = 1, x_4 = 0\) and \(x_3 = 0, x_4 = 1\). What we will get is the special solution for the nullspace of \(M\). 
\[
    Mx = 0
    R = 
    \begin{bmatrix}
        1 & 0 & 2 & 0  \\
        0 & 1 & 0 & 2  \\
    \end{bmatrix}
    s_1 = 
    \begin{bmatrix}
        -2 \\
        0 \\
        1\\
        0\\
    \end{bmatrix}
    \text{ and }
    s_2 = 
    \begin{bmatrix}
         0 \\
         -2 \\
         0 \\
         1 \\
    \end{bmatrix}
\]  

\subsection{Block Elimination in Three Steps: Final Thoughts}

We will conclude nicely with three steps to block elimination.

\textbf{Step 1} Exchange the columns and rows of \(P_C\) and \(P_R\) so that that \(r\) independent columns and rows come first in \(P_{R}AP_C \)
\[
    P_{R}AP_C = 
    \begin{bmatrix}
        W & H  \\
        J & K  \\
    \end{bmatrix}
    , C = 
    \begin{bmatrix}
         W \\
         J \\
    \end{bmatrix}
    \text{ and }
    , B = 
    \begin{bmatrix}
        W & H  \\
    \end{bmatrix}
\]    

\textbf{Step 2} Multiple the top rows by \(W^{-1}\) to produce \(W^{-1}B = [I, W^{-1}H] = [I, F]\).

\textbf{Step 3} Subtract \(J[I, W^{-1}H]\) from \([J, K]\) to produce \([0, 0]\). 

\textbf{The results of the steps should be an rref form of \(R_0\)}
\[
    P_{R}AP_{C} = 
    \begin{bmatrix}
        W & H  \\
        J & K  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        I & W^{-1}H  \\
        J & K  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        I & W^{-1}H   \\
        0 & 0  \\
    \end{bmatrix}
    = R_0
\] 

There are two things that need to be remembered. 
\begin{enumerate}
    \item \(W\)  is invertible
    \item The block satisfies \(JW^{-1}H = K\) 
\end{enumerate}

1. We must think back to \(A = CR\). We can see that \(B = WR\), and since \(B\) and \(R\) have the rank of \(r\) and \(W\) is also \(r \text{ by } r\), that means that \(W\) must have a rank of \(r\) and be invertible. 

2. We know that the first row \([I, W^{-1}H]\) is linearly independent. Since \(A\) has the rank \(r\), it means that the lower row \([J, K]\) must be a combination of the upper rows. This means for the combination to be valid, \(JI = J\) and \(JW^{-1}H = K\). 
\[
    \text{The conclusion is that }
    P_{R}AP_{C} = 
    \begin{bmatrix}
         W \\
         J \\
    \end{bmatrix}
    W^{-1}
    \begin{bmatrix}
        W & H  \\
    \end{bmatrix}
    = CW^{-1}B.
\] 

\section{The Complete Solution to \(Ax = b\)}

Our goal in this section will be about: 
\begin{enumerate}
    \item The complete solution to \(Ax = b\): \(x = x_p + x_n\), where p starts for any particular x and \(n\) nullspace. 
    \item Elimination from \(Ax = b\) to \(R_{0}x = d\): Solvable when zero rows of \(R_0\) have zero in \(d\).   
    \item When \(R_{0}x = d\) is solvable, one \(x_p\) has all free variable equal to zero. 
    \item A has full column rank \(r = n\) when its nullspace \(N(A)\) = zero vector: no free variables. 
    \item A has full row rank \(r = m\) when its column space \(C(A)\) is \(R^m\): \(Ax = b\) is always solvable.         
\end{enumerate}

The biggest thing that changed is that the \(b\) of \(Ax = b\) is now not zero. Therefore, the row operation will also act on the right side, the \(b\) side. \(Ax = b\) is reduced to a simpler \(R_{0}x = d\) with the same, if any, solutions. One way to organize that is by augmenting (adding another column to the right side of the matrix). We can augment \(A\) with the right side of \((b_1, b_2, b_3) = (1, 6, 7)\) to produce augmented matrix \([A b]\)         

\[
    \begin{bmatrix}
        1 & 3 & 0 & 2  \\
        0 & 0 & 1 & 4  \\
        1 & 3 & 1 & 6  \\
    \end{bmatrix}
    \begin{bmatrix}
         x_1 \\
        x_2  \\
        x_3  \\
        x_4  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         1 \\
         6 \\
         7 \\
    \end{bmatrix}
    \text{ has the augmented matrix}
    \begin{bmatrix}
        1 & 3 & 0 & 2  & 1  \\
        0 & 0 & 1 & 4 & 6  \\
        1 & 3 & 1 & 6 & 7  \\
    \end{bmatrix}
    = [A b]
\]

Now if we turn it into its rref form 

\[
    \begin{bmatrix}
        1 & 3 & 0 & 2  \\
        0 & 0 & 1 & 4  \\
        0 & 0 & 0 & 0  \\
    \end{bmatrix}
    \begin{bmatrix}
         x_1 \\
        x_2  \\
        x_3  \\
        x_4  \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         1 \\
         6 \\
         0 \\
    \end{bmatrix}
    \text{ has the augmented matrix}
    \begin{bmatrix}
        1 & 3 & 0 & 2  & 1  \\
        0 & 0 & 1 & 4 & 6  \\
        0 & 0 & 0 & 0 & 0  \\
    \end{bmatrix}
    = [R_0 d]
\]
The last row is very important. Third equation became 0 = 0, each means it can be solved. In the original matrix, the first row plus the second row equals the third row. Meaning to solve \(Ax = b\) we need \(b_1 + b_2 = b_3\), which led to \(0 = 0\) in the third equation. 

\subsection{One Particular Solution Ax_p = b}

To get an easy \(x_p\), let the free variables be zeros: \(x_2 = x_4 = 0\), and the two nonzero equations be the two pivot variables \(x_1 = 1, x_3 = 6\). So our \(x_p\) is \(x_p = (1, 0, 6, 0)\). To put it simply, \textbf{free variables = zero, pivots = variable from d}. 

For a solution to exist, zero rows in \(R_0\) must also be zero in \(d\). 

\[
    R_{0}x_p = 
    \begin{bmatrix}
        1 & 3 & 0 & 2  \\
        0 & 0 & 1 & 4  \\
        0 & 0 & 0 & 0  \\
    \end{bmatrix} 
    \begin{bmatrix}
         1 \\
         0 \\
          6\\
          0\\
    \end{bmatrix}
    = 
    \begin{bmatrix}
         1 \\
          6\\
          0\\
    \end{bmatrix}
\]

Notice how the complete solution includes all \(x_n\):
\[
    x = x_p + x_n = 
    \begin{bmatrix}
         1 \\
         0 \\
         6 \\
         0 \\
    \end{bmatrix}
    + x_2
    \begin{bmatrix}
         -3 \\
         1 \\
         0 \\
         0 \\
    \end{bmatrix}
    +
    \begin{bmatrix}
        -2  \\
        0  \\
        -4  \\
        1  \\
    \end{bmatrix}
\] 

\textbf{Example 1}
Find the condition on \(b_1, b_2, b_3\) for \(Ax = b\) for 
\[
    A = 
    \begin{bmatrix}
        1 & 1  \\
        1 & 2  \\
        -2 & -3  \\
    \end{bmatrix}
    \text{ and }
    \begin{bmatrix}
         b_1 \\
         b_2 \\
         b_3 \\
    \end{bmatrix}
\]   
\textbf{Solution} Elimination using augmented matrix \([A, b]\)
\[
    \begin{bmatrix}
        1 & 1 & b_1  \\
        1 & 2 & b_2  \\
        -2 & -3 & b_3  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 1 & b_1  \\
        0 & 1 & b_2 - b_1  \\
        0 & -1 & b_3 + 2b_1  \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
        1 & 0 & 2b_{1} - b_2  \\
        0 & 1 & b_2 - b_1  \\
        0 & 0 & b_3 + b_1 + b_2  \\
    \end{bmatrix}
    = 
    [R_0 d ]
\] 

And since there is no special solution \((n - r = 2 - 2 = 0)\), the nullspace solution is \(x_n = 0\). So the complete solution is 
\[
    x = x_p + x_n = 
    \begin{bmatrix}
         2b_1 - b2 \\
         b_2 - b_1 \\
    \end{bmatrix}
    + 
    \begin{bmatrix}
         0 \\
         0 \\
    \end{bmatrix}
\]  

\textbf{Every matrix \(A\) with full column rank \((r = n) has these properties:\)  }
\begin{enumerate}
    \item All columns are independent, no free variables. 
    \item The nullspace \(N(A)\) include only the zero vector \(x = 0\)
    \item If \(Ax = b\) has a solution, then it has only one solution.    
\end{enumerate} 
With full column rank, \(Ax = b\) will have \textbf{one or no solution only.}

\subsection{Full Row Rank and the Complete Solution}

Another extreme case is the full row rank. Now \(Ax = b\) has one or infinitely many solutions. Full row rank requires that the matrix be a short and wide one (m > n, row q > col q), and every row is independent. 
For \textbf{example 2}, \(Ax = b\) has 3 n but only two m. 
\[
    \begin{bmatrix}
        1 & 1 & 1 & 3  \\
        1 & 2 & -1 & 4  \\
    \end{bmatrix}
    \text{ rank r = m = 2}
\]   

Imagine them as plane in \(xyz\) space. There are two planes, colliding into a line. The particular solution is a spot on that line, and the nullspace vector will move us along that line. \(x = x_p + \text{ all } x_n\) gives us the whole line solution. 

Fast forward a bit, getting ex. 2 into the \([A b]\) form gives us:
\[
    \begin{bmatrix}
        1 & 0 & 3 & 2  \\
        0 & 1 & -2 & 1  \\
    \end{bmatrix}
    = [R, d]
\] 
This particular solution \((2, 1, 0)\) has free variable \(x_3 = 0\). The special solution has \(s_3 = 1\), and the \(-x_1\) and \(-x_2\) comes from the free column of R. 

Check that \(x_p\) and \(s\) satisfies \(Ax_p = b\) and \(As = 0\)
\[
    2 + 1 = 3, 2 + 2 = 4
    -3 + 2 + 1 = 0, -3 + 4 - 1 = 0
\]    
Remove that the nullspace solution \(x_n\) is just any multiple of \(s\). 
\[
    \text{Computer solution: } 
    x = x_p + x_n = 
    \begin{bmatrix}
         2 \\
         1 \\
         0 \\
    \end{bmatrix}
    + x_3
    \begin{bmatrix}
         -3 \\
         2 \\
         1 \\
    \end{bmatrix}
\]  

Every matrix \(A\) with a full row rank \((r = m)\) has all these properties: 
\begin{enumerate}
    \item All rows have pivot and \(R_0\) has no zero rows: \(R_0 = R\)
    \item \(Ax = b\) has a solution for every right side \(b\).
    \item The column space of \(A\) is the whole space \(R^m\).
    \item If \(m < n\) , the equation has many solutions (called underdetermined in formal language).    
    \item The rows are linearly independent.  
\end{enumerate}  

There are \textbf{four} possibilities for linear equation depend on rank \(r\)
\begin{enumerate}
    \item \(r = m\) and \(r = n\) Square ad invertible, \(Ax = b\) has 1 solution. 
    \item \(r = m\) and \(r < n\) Short and wide, \(Ax = b\) has infinite solutions.    
    \item \(r < m\) and \(r = n\) Tall and thin, \(Ax = b\) has 0 or 1 solutions. 
    \item \(r < m\) and \(r < n\) Not full rank, \(Ax = b\) has 0 or infinite solutions.        
\end{enumerate} 

The reduced \(R_0\) will fall in the same category as matrix A. For \(R_{0}x = d\) and \(Ax = b\) to be solvable, \(d\) must end in \( m - r\) zeros. 

\textbf{Four types for \(R_0\) }
\[
    \begin{bmatrix}
         I \\
    \end{bmatrix}
    , r = m = n
\] 
\[
    \begin{bmatrix}
        I & F  \\
    \end{bmatrix}
    r = m < n 
\]
\[
    \begin{bmatrix}
         I \\
         0 \\
    \end{bmatrix}
    r = n < m
\]
\[
    \begin{bmatrix}
        I & F  \\
        0 & 0  \\
    \end{bmatrix}
    r < m, r , n
\]

Case 1 and 2 have full row rank \(r = m\). Case 1 and 3 have full column rank \(r = n\).  

\section{Independence, Basis, Dimension}

How big is the true size of a subspace? A matrix might be \(m\) by \(n\) but the column space is not necessarily \(n\). The column space \(C(A)\) is measured by independent columns. This will be clarified later. 

Our goal here is to understand a \textbf{basis: independent vectors that "spans" a space.} Every vector in the space is an unique combination of the basis vectors. 
Some vague explanation of the terms:
\begin{enumerate}
    \item Independent vectors, no extra vectors 
    \item Spanning a space, enough vectors to produce the rest. 
    \item Basis for a space, not too many and not too few 
    \item Dimension of a space, the number of vectors in every basis. 
\end{enumerate}

\section{Dimensions of the Four Subspaces}

